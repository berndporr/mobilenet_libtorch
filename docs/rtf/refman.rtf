{\rtf1\ansi\ansicpg1252\uc1 \deff0\deflang1033\deflangfe1033
{\fonttbl {\f0\froman\fcharset0\fprq2{\*\panose 02020603050405020304}Times New Roman;}
{\f1\fswiss\fcharset0\fprq2{\*\panose 020b0604020202020204}Arial;}
{\f2\fmodern\fcharset0\fprq1{\*\panose 02070309020205020404}Courier New;}
{\f3\froman\fcharset2\fprq2{\*\panose 05050102010706020507}Symbol;}
}
{\colortbl;\red0\green0\blue0;\red0\green0\blue255;\red0\green255\blue255;\red0\green255\blue0;\red255\green0\blue255;\red255\green0\blue0;\red255\green255\blue0;\red255\green255\blue255;\red0\green0\blue128;\red0\green128\blue128;\red0\green128\blue0;\red128\green0\blue128;\red128\green0\blue0;\red128\green128\blue0;\red128\green128\blue128;\red192\green192\blue192;\red0\green128\blue0;\red96\green64\blue32;\rede0\green128\blue0;\red128\green0\blue0;\red128\green96\blue32;\red0\green32\blue128;\red0\green128\blue128;\red255\green0\blue255;\red0\green0\blue0;\red112\green0\blue112;\red255\green0\blue0;}
{\stylesheet
{\widctlpar\adjustright \fs20\cgrid \snext0 Normal;}
{\paperw11900\paperh16840\margl1800\margr1800\margt1440\margb1440\gutter0\ltrsect}
{\s1\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs36\kerning36\cgrid \sbasedon0 \snext0 heading 1;}
{\s2\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs28\kerning28\cgrid \sbasedon0 \snext0 heading 2;}
{\s3\sb240\sa60\keepn\widctlpar\adjustright \b\f1\cgrid \sbasedon0 \snext0 heading 3;}
{\s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid \sbasedon0 \snext0 heading 4;}{\*\cs10 \additive Default Paragraph Font;}
{\s5\sb90\sa30\keepn\widctlpar\adjustright \b\f1\fs20\cgrid \sbasedon0 \snext0 heading 5;}{\*\cs10 \additive Default Paragraph Font;}
{\s15\qc\sb240\sa60\widctlpar\outlinelevel0\adjustright \b\f1\fs32\kerning28\cgrid \sbasedon0 \snext15 Title;}
{\s16\qc\sa60\widctlpar\outlinelevel1\adjustright \f1\cgrid \sbasedon0 \snext16 Subtitle;}
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid \sbasedon0 \snext17 BodyText;}
{\s18\widctlpar\fs22\cgrid \sbasedon0 \snext18 DenseText;}
{\s28\widctlpar\tqc\tx4320\tqr\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext28 header;}
{\s29\widctlpar\tqc\tx4320\tqr\tx8640\qr\adjustright \fs20\cgrid \sbasedon0 \snext29 footer;}
{\s30\li360\sa60\sb120\keepn\widctlpar\adjustright \b\f1\fs20\cgrid \sbasedon0 \snext30 GroupHeader;}
{\s40\li0\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext41 Code Example 0;}
{\s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext42 Code Example 1;}
{\s42\li720\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext43 Code Example 2;}
{\s43\li1080\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext44 Code Example 3;}
{\s44\li1440\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext45 Code Example 4;}
{\s45\li1800\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext46 Code Example 5;}
{\s46\li2160\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext47 Code Example 6;}
{\s47\li2520\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext48 Code Example 7;}
{\s48\li2880\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext49 Code Example 8;}
{\s49\li3240\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext50 Code Example 9;}
{\s50\li3600\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext51 Code Example 10;}
{\s51\li3960\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext52 Code Example 11;}
{\s52\li4320\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext53 Code Example 12;}
{\s53\li4680\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext53 Code Example 13;}
{\s60\li0\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext61 List Continue 0;}
{\s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext62 List Continue 1;}
{\s62\li720\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext63 List Continue 2;}
{\s63\li1080\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext64 List Continue 3;}
{\s64\li1440\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext65 List Continue 4;}
{\s65\li1800\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext66 List Continue 5;}
{\s66\li2160\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext67 List Continue 6;}
{\s67\li2520\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext68 List Continue 7;}
{\s68\li2880\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext69 List Continue 8;}
{\s69\li3240\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext70 List Continue 9;}
{\s70\li3600\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext71 List Continue 10;}
{\s71\li3960\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext72 List Continue 11;}
{\s72\li4320\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext73 List Continue 12;}
{\s73\li4680\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext73 List Continue 13;}
{\s80\li0\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext81 DescContinue 0;}
{\s81\li360\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext82 DescContinue 1;}
{\s82\li720\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext83 DescContinue 2;}
{\s83\li1080\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext84 DescContinue 3;}
{\s84\li1440\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext85 DescContinue 4;}
{\s85\li1800\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext86 DescContinue 5;}
{\s86\li2160\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext87 DescContinue 6;}
{\s87\li2520\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext88 DescContinue 7;}
{\s88\li2880\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext89 DescContinue 8;}
{\s89\li3240\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext90 DescContinue 9;}
{\s90\li3600\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext91 DescContinue 10;}
{\s91\li3960\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext92 DescContinue 11;}
{\s92\li4320\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext93 DescContinue 12;}
{\s93\li4680\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext93 DescContinue 13;}
{\s100\li0\sa30\sb30\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext101 LatexTOC 0;}
{\s101\li360\sa27\sb27\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext102 LatexTOC 1;}
{\s102\li720\sa24\sb24\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext103 LatexTOC 2;}
{\s103\li1080\sa21\sb21\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext104 LatexTOC 3;}
{\s104\li1440\sa18\sb18\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext105 LatexTOC 4;}
{\s105\li1800\sa15\sb15\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext106 LatexTOC 5;}
{\s106\li2160\sa12\sb12\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext107 LatexTOC 6;}
{\s107\li2520\sa9\sb9\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext108 LatexTOC 7;}
{\s108\li2880\sa6\sb6\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext109 LatexTOC 8;}
{\s109\li3240\sa3\sb3\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext110 LatexTOC 9;}
{\s110\li3600\sa3\sb3\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext111 LatexTOC 10;}
{\s111\li3960\sa3\sb3\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext112 LatexTOC 11;}
{\s112\li4320\sa3\sb3\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext113 LatexTOC 12;}
{\s113\li4680\sa3\sb3\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext113 LatexTOC 13;}
{\s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext121 \sautoupd List Bullet 0;}
{\s121\fi-360\li720\widctlpar\jclisttab\tx720{\*\pn \pnlvlbody\ilvl0\ls2\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext122 \sautoupd List Bullet 1;}
{\s122\fi-360\li1080\widctlpar\jclisttab\tx1080{\*\pn \pnlvlbody\ilvl0\ls3\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext123 \sautoupd List Bullet 2;}
{\s123\fi-360\li1440\widctlpar\jclisttab\tx1440{\*\pn \pnlvlbody\ilvl0\ls4\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext124 \sautoupd List Bullet 3;}
{\s124\fi-360\li1800\widctlpar\jclisttab\tx1800{\*\pn \pnlvlbody\ilvl0\ls5\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext125 \sautoupd List Bullet 4;}
{\s125\fi-360\li2160\widctlpar\jclisttab\tx2160{\*\pn \pnlvlbody\ilvl0\ls6\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext126 \sautoupd List Bullet 5;}
{\s126\fi-360\li2520\widctlpar\jclisttab\tx2520{\*\pn \pnlvlbody\ilvl0\ls7\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext127 \sautoupd List Bullet 6;}
{\s127\fi-360\li2880\widctlpar\jclisttab\tx2880{\*\pn \pnlvlbody\ilvl0\ls8\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext128 \sautoupd List Bullet 7;}
{\s128\fi-360\li3240\widctlpar\jclisttab\tx3240{\*\pn \pnlvlbody\ilvl0\ls9\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext129 \sautoupd List Bullet 8;}
{\s129\fi-360\li3600\widctlpar\jclisttab\tx3600{\*\pn \pnlvlbody\ilvl0\ls10\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext130 \sautoupd List Bullet 9;}
{\s130\fi-360\li3960\widctlpar\jclisttab\tx3960{\*\pn \pnlvlbody\ilvl0\ls11\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext131 \sautoupd List Bullet 10;}
{\s131\fi-360\li4320\widctlpar\jclisttab\tx4320{\*\pn \pnlvlbody\ilvl0\ls12\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext132 \sautoupd List Bullet 11;}
{\s132\fi-360\li4680\widctlpar\jclisttab\tx4680{\*\pn \pnlvlbody\ilvl0\ls13\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext133 \sautoupd List Bullet 12;}
{\s133\fi-360\li5040\widctlpar\jclisttab\tx5040{\*\pn \pnlvlbody\ilvl0\ls14\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext133 \sautoupd List Bullet 13;}
{\s140\fi-360\li360\widctlpar\fs20\cgrid \sbasedon0 \snext141 \sautoupd List Enum 0;}
{\s141\fi-360\li720\widctlpar\fs20\cgrid \sbasedon0 \snext142 \sautoupd List Enum 1;}
{\s142\fi-360\li1080\widctlpar\fs20\cgrid \sbasedon0 \snext143 \sautoupd List Enum 2;}
{\s143\fi-360\li1440\widctlpar\fs20\cgrid \sbasedon0 \snext144 \sautoupd List Enum 3;}
{\s144\fi-360\li1800\widctlpar\fs20\cgrid \sbasedon0 \snext145 \sautoupd List Enum 4;}
{\s145\fi-360\li2160\widctlpar\fs20\cgrid \sbasedon0 \snext146 \sautoupd List Enum 5;}
{\s146\fi-360\li2520\widctlpar\fs20\cgrid \sbasedon0 \snext147 \sautoupd List Enum 6;}
{\s147\fi-360\li2880\widctlpar\fs20\cgrid \sbasedon0 \snext148 \sautoupd List Enum 7;}
{\s148\fi-360\li3240\widctlpar\fs20\cgrid \sbasedon0 \snext149 \sautoupd List Enum 8;}
{\s149\fi-360\li3600\widctlpar\fs20\cgrid \sbasedon0 \snext150 \sautoupd List Enum 9;}
{\s150\fi-360\li3960\widctlpar\fs20\cgrid \sbasedon0 \snext151 \sautoupd List Enum 10;}
{\s151\fi-360\li4320\widctlpar\fs20\cgrid \sbasedon0 \snext152 \sautoupd List Enum 11;}
{\s152\fi-360\li4680\widctlpar\fs20\cgrid \sbasedon0 \snext153 \sautoupd List Enum 12;}
{\s153\fi-360\li5040\widctlpar\fs20\cgrid \sbasedon0 \snext153 \sautoupd List Enum 13;}
}
{\info 
{\title {\comment Mobilenet with Libtorch }Mobilenet with Libtorch}
{\comment Generated by doxygen 1.9.8.}
}\pard\plain 
\sectd\pgnlcrm
{\footer \s29\widctlpar\tqc\tx4320\tqr\tx8640\qr\adjustright \fs20\cgrid {\chpgn}}
\pard\plain \s16\qc\sa60\widctlpar\outlinelevel1\adjustright \f1\cgrid 
\vertalc\qc\par\par\par\par\par\par\par
\pard\plain \s15\qc\sb240\sa60\widctlpar\outlinelevel0\adjustright \b\f1\fs32\kerning28\cgrid 
{\field\fldedit {\*\fldinst TITLE \\*MERGEFORMAT}{\fldrslt Mobilenet with Libtorch}}\par
\pard\plain \s16\qc\sa60\widctlpar\outlinelevel1\adjustright \f1\cgrid 
\par
\par\par\par\par\par\par\par\par\par\par\par\par
\pard\plain \s16\qc\sa60\widctlpar\outlinelevel1\adjustright \f1\cgrid 
{\field\fldedit {\*\fldinst AUTHOR \\*MERGEFORMAT}{\fldrslt AUTHOR}}\par
Version \par\page\page\vertalt
\pard\plain 
\s1\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs36\kerning36\cgrid Table of Contents\par
\pard\plain \par
{\field\fldedit {\*\fldinst TOC \\f \\*MERGEFORMAT}{\fldrslt Table of contents}}\par
\pard\plain 
\sect \sbkpage \pgndec \pgnrestart
\sect \sectd \sbknone
{\footer \s29\widctlpar\tqc\tx4320\tqr\tx8640\qr\adjustright \fs20\cgrid {\chpgn}}

\pard\plain \sect\sbkpage
\s1\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs36\kerning36\cgrid 
\par \pard\plain 
\pard\plain \s1\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs36\kerning36\cgrid 
MobileNetV2 libtorch\par \pard\plain 
{\tc\tcl1 \v MobileNetV2 libtorch}
{\xe \v MobileNetV2 libtorch}
{\bkmkstart AAAAAAAAAX}
{\bkmkend AAAAAAAAAX}
{
\pard\plain \s17\sa60\sb30\widctlpar\qj \fs22\cgrid {\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
{\bkmkstart AAAAAAAAAY}
{\bkmkend AAAAAAAAAY}
C++ version of {\b MobileNetV2} using libtorch which can import the pre-trained weights from torchvision.\par
Importing the weights instead of using the JIT has the advantage that one can do transfer learning also on an edge device, for example, to adapt to different situations locally.\par
This implementation follows very closely the {\f2 torchvision implementation of mobilenet v2}.\par
Mobilenet is described {\f2 here}.\par
Then simply include {\f2 {\b mobilenet_v2.h}}  into your own project.\par
{\pard\plain \s1\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs36\kerning36\cgrid {\tc\tcl1 Credit} \par}
(C) 2025 {\f2 Bernd Porr}, GPLv3 \par
}}

\pard\plain \sect\sbkpage
\s1\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs36\kerning36\cgrid 
Hierarchical Index\par \pard\plain 
{\tc \v Hierarchical Index}
\pard\plain \s2\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs28\kerning28\cgrid 
Class Hierarchy\par \pard\plain 
{
\pard\plain \s17\sa60\sb30\widctlpar\qj \fs22\cgrid This inheritance list is sorted roughly, but not completely, alphabetically:}
{
\par
\pard\plain \s101\li360\sa27\sb27\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid 
torch::nn::Module
{
\par
\pard\plain \s102\li720\sa24\sb24\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid 
Conv2dNormActivation\tab {\field\fldedit {\*\fldinst PAGEREF AAAAAAAAAB \\*MERGEFORMAT}{\fldrslt pagenum}}
\par
InvertedResidual\tab {\field\fldedit {\*\fldinst PAGEREF AAAAAAAAAG \\*MERGEFORMAT}{\fldrslt pagenum}}
\par
MobileNetV2\tab {\field\fldedit {\*\fldinst PAGEREF AAAAAAAAAM \\*MERGEFORMAT}{\fldrslt pagenum}}
\par
}\par
}
\pard\plain \sect\sbkpage
\s1\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs36\kerning36\cgrid 
Class Index\par \pard\plain 
{\tc \v Class Index}
\pard\plain \s2\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs28\kerning28\cgrid 
Class List\par \pard\plain 
{
\pard\plain \s17\sa60\sb30\widctlpar\qj \fs22\cgrid Here are the classes, structs, unions and interfaces with brief descriptions:}
{
\par
\pard\plain \s101\li360\sa27\sb27\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid 
{\b {\b Conv2dNormActivation} ({\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Module which performs Convolution, Batch Norm and Relu6 })} \tab {\field\fldedit {\*\fldinst PAGEREF AAAAAAAAAB \\*MERGEFORMAT}{\fldrslt pagenum}}
\par
{\b {\b InvertedResidual} ({\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Inverted residual Converted from {\f2 https://github.com/pytorch/vision/blob/main/torchvision/models/mobilenetv2.py#L19} })} \tab {\field\fldedit {\*\fldinst PAGEREF AAAAAAAAAG \\*MERGEFORMAT}{\fldrslt pagenum}}
\par
{\b {\b MobileNetV2} ({\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Implementation of {\b MobileNetV2} as done in py-torchvision See: // {\f2 https://github.com/pytorch/vision/blob/main/torchvision/models/mobilenetv2.py#L66} })} \tab {\field\fldedit {\*\fldinst PAGEREF AAAAAAAAAM \\*MERGEFORMAT}{\fldrslt pagenum}}
\par
\par}
\pard\plain \sect\sbkpage
\s1\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs36\kerning36\cgrid 
File Index\par \pard\plain 
{\tc \v File Index}
\pard\plain \s2\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs28\kerning28\cgrid 
File List\par \pard\plain 
{
\pard\plain \s17\sa60\sb30\widctlpar\qj \fs22\cgrid Here is a list of all documented files with brief descriptions:}
{
\par
\pard\plain \s101\li360\sa27\sb27\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid 
{\b {\b mobilenet_v2.h} } \tab {\field\fldedit {\*\fldinst PAGEREF AAAAAAAAAA \\*MERGEFORMAT}{\fldrslt pagenum}}
\par
\par}
\pard\plain \sect\sbkpage
\s1\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs36\kerning36\cgrid 
Class Documentation{\tc \v Class Documentation}
\par \pard\plain 
\pard\plain \s2\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs28\kerning28\cgrid 
Conv2dNormActivation Class Reference\par \pard\plain 
{\tc\tcl2 \v Conv2dNormActivation}
{\xe \v Conv2dNormActivation}
{\bkmkstart AAAAAAAAAB}
{\bkmkend AAAAAAAAAB}
\par
{
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Module which performs Convolution, Batch Norm and Relu6. }}\par
{
{\f2 #include <mobilenet_v2.h>}}\par
Inheritance diagram for Conv2dNormActivation:{
\pard\plain 
\par\pard \qc {\field\flddirty {\*\fldinst INCLUDEPICTURE "classConv2dNormActivation__inherit__graph.png" \\d \\*MERGEFORMAT}{\fldrslt IMAGE}}\par
}
\par
Collaboration diagram for Conv2dNormActivation:{
\pard\plain 
\par\pard \qc {\field\flddirty {\*\fldinst INCLUDEPICTURE "classConv2dNormActivation__coll__graph.png" \\d \\*MERGEFORMAT}{\fldrslt IMAGE}}\par
}
\par
\pard\plain \s3\sb240\sa60\keepn\widctlpar\adjustright \b\f1\cgrid 
Public Member Functions\par
\pard\plain 

{
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

{\b Conv2dNormActivation} (int in_channels, int out_channels, int kernel_size=3, int stride=1, int padding=-1, int groups=1){\bkmkstart AAAAAAAAAC}
{\bkmkend AAAAAAAAAC}
\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

torch::Tensor {\b forward} (const torch::Tensor &x){\bkmkstart AAAAAAAAAD}
{\bkmkend AAAAAAAAAD}
\par
}
\pard\plain \s3\sb240\sa60\keepn\widctlpar\adjustright \b\f1\cgrid 
Static Public Member Functions\par
\pard\plain 

{
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

static torch::Tensor {\b relu6} (const torch::Tensor &x){\bkmkstart AAAAAAAAAE}
{\bkmkend AAAAAAAAAE}
\par
}
\pard\plain \s3\sb240\sa60\keepn\widctlpar\adjustright \b\f1\cgrid 
Static Public Attributes\par
\pard\plain 

{
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

static constexpr char {\b className} [] = "Conv2dNormActivation"{\bkmkstart AAAAAAAAAF}
{\bkmkend AAAAAAAAAF}
\par
}
{\pard\widctlpar\brdrb\brdrs\brdrw5\brsp20 \adjustright \par}
\pard\plain \s3\sb240\sa60\keepn\widctlpar\adjustright \b\f1\cgrid 
Detailed Description\par
\pard\plain 
{
\pard\plain \s17\sa60\sb30\widctlpar\qj \fs22\cgrid {\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Module which performs Convolution, Batch Norm and Relu6. \par
}

{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
See {\f2 https://github.com/pytorch/vision/blob/main/torchvision/ops/misc.py#L126} \par
}}
{\pard\widctlpar\brdrb\brdrs\brdrw5\brsp20 \adjustright \par}
The documentation for this class was generated from the following file:{\par
\pard\plain \s121\fi-360\li720\widctlpar\jclisttab\tx720{\*\pn \pnlvlbody\ilvl0\ls2\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
mobilenet_v2.h\par
}\par \pard\plain 

\pard\plain \sect\sbkpage
\s2\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs28\kerning28\cgrid 
\pard\plain \s2\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs28\kerning28\cgrid 
InvertedResidual Class Reference\par \pard\plain 
{\tc\tcl2 \v InvertedResidual}
{\xe \v InvertedResidual}
{\bkmkstart AAAAAAAAAG}
{\bkmkend AAAAAAAAAG}
\par
{
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Inverted residual Converted from {\f2 https://github.com/pytorch/vision/blob/main/torchvision/models/mobilenetv2.py#L19}. }}\par
{
{\f2 #include <mobilenet_v2.h>}}\par
Inheritance diagram for InvertedResidual:{
\pard\plain 
\par\pard \qc {\field\flddirty {\*\fldinst INCLUDEPICTURE "classInvertedResidual__inherit__graph.png" \\d \\*MERGEFORMAT}{\fldrslt IMAGE}}\par
}
\par
Collaboration diagram for InvertedResidual:{
\pard\plain 
\par\pard \qc {\field\flddirty {\*\fldinst INCLUDEPICTURE "classInvertedResidual__coll__graph.png" \\d \\*MERGEFORMAT}{\fldrslt IMAGE}}\par
}
\par
\pard\plain \s3\sb240\sa60\keepn\widctlpar\adjustright \b\f1\cgrid 
Public Member Functions\par
\pard\plain 

{
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

{\b InvertedResidual} (int inp, int oup, int stride, int expand_ratio){\bkmkstart AAAAAAAAAH}
{\bkmkend AAAAAAAAAH}
\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

torch::Tensor {\b forward} (const torch::Tensor &x){\bkmkstart AAAAAAAAAI}
{\bkmkend AAAAAAAAAI}
\par
}
\pard\plain \s3\sb240\sa60\keepn\widctlpar\adjustright \b\f1\cgrid 
Public Attributes\par
\pard\plain 

{
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

torch::nn::Sequential {\b conv} \{nullptr\}{\bkmkstart AAAAAAAAAJ}
{\bkmkend AAAAAAAAAJ}
\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

bool {\b use_res_connect}{\bkmkstart AAAAAAAAAK}
{\bkmkend AAAAAAAAAK}
\par
}
\pard\plain \s3\sb240\sa60\keepn\widctlpar\adjustright \b\f1\cgrid 
Static Public Attributes\par
\pard\plain 

{
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

static constexpr char {\b className} [] = "InvertedResidual"{\bkmkstart AAAAAAAAAL}
{\bkmkend AAAAAAAAAL}
\par
}
{\pard\widctlpar\brdrb\brdrs\brdrw5\brsp20 \adjustright \par}
\pard\plain \s3\sb240\sa60\keepn\widctlpar\adjustright \b\f1\cgrid 
Detailed Description\par
\pard\plain 
{
\pard\plain \s17\sa60\sb30\widctlpar\qj \fs22\cgrid {\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Inverted residual Converted from {\f2 https://github.com/pytorch/vision/blob/main/torchvision/models/mobilenetv2.py#L19}. \par
}}
{\pard\widctlpar\brdrb\brdrs\brdrw5\brsp20 \adjustright \par}
The documentation for this class was generated from the following file:{\par
\pard\plain \s121\fi-360\li720\widctlpar\jclisttab\tx720{\*\pn \pnlvlbody\ilvl0\ls2\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
mobilenet_v2.h\par
}\par \pard\plain 

\pard\plain \sect\sbkpage
\s2\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs28\kerning28\cgrid 
\pard\plain \s2\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs28\kerning28\cgrid 
MobileNetV2 Class Reference\par \pard\plain 
{\tc\tcl2 \v MobileNetV2}
{\xe \v MobileNetV2}
{\bkmkstart AAAAAAAAAM}
{\bkmkend AAAAAAAAAM}
\par
{
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Implementation of {\b MobileNetV2} as done in py-torchvision See: // {\f2 https://github.com/pytorch/vision/blob/main/torchvision/models/mobilenetv2.py#L66}. }}\par
{
{\f2 #include <mobilenet_v2.h>}}\par
Inheritance diagram for MobileNetV2:{
\pard\plain 
\par\pard \qc {\field\flddirty {\*\fldinst INCLUDEPICTURE "classMobileNetV2__inherit__graph.png" \\d \\*MERGEFORMAT}{\fldrslt IMAGE}}\par
}
\par
Collaboration diagram for MobileNetV2:{
\pard\plain 
\par\pard \qc {\field\flddirty {\*\fldinst INCLUDEPICTURE "classMobileNetV2__coll__graph.png" \\d \\*MERGEFORMAT}{\fldrslt IMAGE}}\par
}
\par
\pard\plain \s3\sb240\sa60\keepn\widctlpar\adjustright \b\f1\cgrid 
Public Member Functions\par
\pard\plain 

{
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\b MobileNetV2} (int num_classes=1000, float width_mult=1.0f, int round_nearest=8, float dropout=0.2)\par
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid {\i {\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Construct a new {\b MobileNetV2} object If you want to load the weights from torchvision into the classifier use the default values for the parameters. }{
}\par
}\par}
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
int {\b getNinputChannels4Classifier} () const\par
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid {\i {\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Gets the number of input channels for the classfier This will make it easy to replace the classifier with anything the user wants by creating their own torch::nn::Sequential() for the classifier. }{
}\par
}\par}
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
torch::Tensor {\b forward} (torch::Tensor x)\par
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid {\i {\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Performs the forward pass. }{
}\par
}\par}
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

void {\b initialize_weights} (){\bkmkstart AAAAAAAAAN}
{\bkmkend AAAAAAAAAN}
\par
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid {\i {\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Initialize conv/bn/linear similar to torchvision defaults. }{
}\par
}\par}
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
void {\b load_weights} (std::string pt)\par
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid {\i {\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Loads a .pt weight file containing a dic with key/parameter pairs See {\f2 https://github.com/pytorch/pytorch/issues/36577}. }{
}\par
}\par}
}
\pard\plain \s3\sb240\sa60\keepn\widctlpar\adjustright \b\f1\cgrid 
Static Public Member Functions\par
\pard\plain 

{
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
static torch::Tensor {\b preprocess} (cv::Mat img, bool resizeOnly=false)\par
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid {\i {\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Preprocessing of an openCV image for inference or learning The images are resized to 256x256, followed by a central crop of 224x224. }{
}\par
}\par}
}
\pard\plain \s3\sb240\sa60\keepn\widctlpar\adjustright \b\f1\cgrid 
Public Attributes\par
\pard\plain 

{
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

torch::nn::Sequential {\b classifier} \{nullptr\}{\bkmkstart AAAAAAAAAO}
{\bkmkend AAAAAAAAAO}
\par
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid {\i {\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Classifier submodule This can be replaced by a custom classifier for transfer learning. }{
}\par
}\par}
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

torch::nn::Sequential {\b features} \{nullptr\}{\bkmkstart AAAAAAAAAP}
{\bkmkend AAAAAAAAAP}
\par
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid {\i {\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Features submodule This needs to be accessible for transfer learning which then will disable learning here. }{
}\par
}\par}
}
\pard\plain \s3\sb240\sa60\keepn\widctlpar\adjustright \b\f1\cgrid 
Static Public Attributes\par
\pard\plain 

{
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

static constexpr char {\b featuresModuleName} [] = "features"{\bkmkstart AAAAAAAAAQ}
{\bkmkend AAAAAAAAAQ}
\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
static constexpr char {\b classifierModuleName} [] = "classifier"\par
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid {\i {\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Name of the classifier module. }{
}\par
}\par}
}
{\pard\widctlpar\brdrb\brdrs\brdrw5\brsp20 \adjustright \par}
\pard\plain \s3\sb240\sa60\keepn\widctlpar\adjustright \b\f1\cgrid 
Detailed Description\par
\pard\plain 
{
\pard\plain \s17\sa60\sb30\widctlpar\qj \fs22\cgrid {\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Implementation of {\b MobileNetV2} as done in py-torchvision See: // {\f2 https://github.com/pytorch/vision/blob/main/torchvision/models/mobilenetv2.py#L66}. \par
}}
{\pard\widctlpar\brdrb\brdrs\brdrw5\brsp20 \adjustright \par}
\pard\plain \s3\sb240\sa60\keepn\widctlpar\adjustright \b\f1\cgrid 
Constructor & Destructor Documentation\par
\pard\plain 
{\xe \v MobileNetV2\:MobileNetV2}
{\xe \v MobileNetV2\:MobileNetV2}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
MobileNetV2::MobileNetV2 (int  {\i num_classes} = {\f2 1000}, float  {\i width_mult} = {\f2 1.0f}, int  {\i round_nearest} = {\f2 8}, float  {\i dropout} = {\f2 0.2}){\f2 [inline]}}}
\par
{\bkmkstart AAAAAAAAAR}
{\bkmkend AAAAAAAAAR}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
\par
{
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Construct a new {\b MobileNetV2} object If you want to load the weights from torchvision into the classifier use the default values for the parameters. }}\par
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
{\par
{\s5\sb90\sa30\keepn\widctlpar\adjustright \b\f1\fs20\cgrid 
Parameters\par}
\pard\plain \s81\li360\widctlpar\ql\adjustright \fs20\cgrid \trowd \trgaph108\trleft426\tblind426\trbrdrt\brdrs\brdrw10\brdrcf15 \trbrdrl\brdrs\brdrw10\brdrcf15 \trbrdrb\brdrs\brdrw10\brdrcf15 \trbrdrr\brdrs\brdrw10\brdrcf15 \trbrdrh\brdrs\brdrw10\brdrcf15 \trbrdrv\brdrs\brdrw10\brdrcf15 
\clvertalt\clbrdrt\brdrs\brdrw10\brdrcf15 \clbrdrl\brdrs\brdrw10\brdrcf15 \clbrdrb\brdrs\brdrw10\brdrcf15 \clbrdrr \brdrs\brdrw10\brdrcf15 \cltxlrtb \cellx2187
\clvertalt\clbrdrt\brdrs\brdrw10\brdrcf15 \clbrdrl\brdrs\brdrw10\brdrcf15 \clbrdrb\brdrs\brdrw10\brdrcf15 \clbrdrr \brdrs\brdrw10\brdrcf15 \cltxlrtb \cellx8748
\pard \widctlpar\intbl\adjustright
{{\i num_classes} \cell }{Number of classes \cell }
{\row }
\trowd \trgaph108\trleft426\tblind426\trbrdrt\brdrs\brdrw10\brdrcf15 \trbrdrl\brdrs\brdrw10\brdrcf15 \trbrdrb\brdrs\brdrw10\brdrcf15 \trbrdrr\brdrs\brdrw10\brdrcf15 \trbrdrh\brdrs\brdrw10\brdrcf15 \trbrdrv\brdrs\brdrw10\brdrcf15 
\clvertalt\clbrdrt\brdrs\brdrw10\brdrcf15 \clbrdrl\brdrs\brdrw10\brdrcf15 \clbrdrb\brdrs\brdrw10\brdrcf15 \clbrdrr \brdrs\brdrw10\brdrcf15 \cltxlrtb \cellx2187
\clvertalt\clbrdrt\brdrs\brdrw10\brdrcf15 \clbrdrl\brdrs\brdrw10\brdrcf15 \clbrdrb\brdrs\brdrw10\brdrcf15 \clbrdrr \brdrs\brdrw10\brdrcf15 \cltxlrtb \cellx8748
\pard \widctlpar\intbl\adjustright
{{\i width_mult} \cell }{Width multiplier - adjusts number of channels in each layer by this amount \cell }
{\row }
\trowd \trgaph108\trleft426\tblind426\trbrdrt\brdrs\brdrw10\brdrcf15 \trbrdrl\brdrs\brdrw10\brdrcf15 \trbrdrb\brdrs\brdrw10\brdrcf15 \trbrdrr\brdrs\brdrw10\brdrcf15 \trbrdrh\brdrs\brdrw10\brdrcf15 \trbrdrv\brdrs\brdrw10\brdrcf15 
\clvertalt\clbrdrt\brdrs\brdrw10\brdrcf15 \clbrdrl\brdrs\brdrw10\brdrcf15 \clbrdrb\brdrs\brdrw10\brdrcf15 \clbrdrr \brdrs\brdrw10\brdrcf15 \cltxlrtb \cellx2187
\clvertalt\clbrdrt\brdrs\brdrw10\brdrcf15 \clbrdrl\brdrs\brdrw10\brdrcf15 \clbrdrb\brdrs\brdrw10\brdrcf15 \clbrdrr \brdrs\brdrw10\brdrcf15 \cltxlrtb \cellx8748
\pard \widctlpar\intbl\adjustright
{{\i round_nearest} \cell }{Round the number of channels in each layer to be a multiple of this number \cell }
{\row }
\trowd \trgaph108\trleft426\tblind426\trbrdrt\brdrs\brdrw10\brdrcf15 \trbrdrl\brdrs\brdrw10\brdrcf15 \trbrdrb\brdrs\brdrw10\brdrcf15 \trbrdrr\brdrs\brdrw10\brdrcf15 \trbrdrh\brdrs\brdrw10\brdrcf15 \trbrdrv\brdrs\brdrw10\brdrcf15 
\clvertalt\clbrdrt\brdrs\brdrw10\brdrcf15 \clbrdrl\brdrs\brdrw10\brdrcf15 \clbrdrb\brdrs\brdrw10\brdrcf15 \clbrdrr \brdrs\brdrw10\brdrcf15 \cltxlrtb \cellx2187
\clvertalt\clbrdrt\brdrs\brdrw10\brdrcf15 \clbrdrl\brdrs\brdrw10\brdrcf15 \clbrdrb\brdrs\brdrw10\brdrcf15 \clbrdrr \brdrs\brdrw10\brdrcf15 \cltxlrtb \cellx8748
\pard \widctlpar\intbl\adjustright
{{\i dropout} \cell }{Dropout probability for the dropout layer in the classifier \cell }
{\row }
}
}}
{\pard\widctlpar\brdrb\brdrs\brdrw5\brsp20 \adjustright \par}
\pard\plain \s3\sb240\sa60\keepn\widctlpar\adjustright \b\f1\cgrid 
Member Function Documentation\par
\pard\plain 
{\xe \v forward\:MobileNetV2}
{\xe \v MobileNetV2\:forward}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
torch::Tensor MobileNetV2::forward (torch::Tensor  {\i x}){\f2 [inline]}}}
\par
{\bkmkstart AAAAAAAAAS}
{\bkmkend AAAAAAAAAS}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
\par
{
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Performs the forward pass. }}\par
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
{\par
{\s5\sb90\sa30\keepn\widctlpar\adjustright \b\f1\fs20\cgrid 
Parameters\par}
\pard\plain \s81\li360\widctlpar\ql\adjustright \fs20\cgrid \trowd \trgaph108\trleft426\tblind426\trbrdrt\brdrs\brdrw10\brdrcf15 \trbrdrl\brdrs\brdrw10\brdrcf15 \trbrdrb\brdrs\brdrw10\brdrcf15 \trbrdrr\brdrs\brdrw10\brdrcf15 \trbrdrh\brdrs\brdrw10\brdrcf15 \trbrdrv\brdrs\brdrw10\brdrcf15 
\clvertalt\clbrdrt\brdrs\brdrw10\brdrcf15 \clbrdrl\brdrs\brdrw10\brdrcf15 \clbrdrb\brdrs\brdrw10\brdrcf15 \clbrdrr \brdrs\brdrw10\brdrcf15 \cltxlrtb \cellx2187
\clvertalt\clbrdrt\brdrs\brdrw10\brdrcf15 \clbrdrl\brdrs\brdrw10\brdrcf15 \clbrdrb\brdrs\brdrw10\brdrcf15 \clbrdrr \brdrs\brdrw10\brdrcf15 \cltxlrtb \cellx8748
\pard \widctlpar\intbl\adjustright
{{\i x} \cell }{The batch of input images. \cell }
{\row }
}
{{\s5\sb90\sa30\keepn\widctlpar\adjustright \b\f1\fs20\cgrid 
Returns\par}\pard\plain \s82\li720\widctlpar\ql\adjustright \fs20\cgrid {\s17 \sa60 \sb30
torch::Tensor The category scores for the different labels. \par
}}}}
{\xe \v getNinputChannels4Classifier\:MobileNetV2}
{\xe \v MobileNetV2\:getNinputChannels4Classifier}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
int MobileNetV2::getNinputChannels4Classifier () const{\f2 [inline]}}}
\par
{\bkmkstart AAAAAAAAAT}
{\bkmkend AAAAAAAAAT}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
\par
{
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Gets the number of input channels for the classfier This will make it easy to replace the classifier with anything the user wants by creating their own torch::nn::Sequential() for the classifier. }}\par
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
\par
{{\s5\sb90\sa30\keepn\widctlpar\adjustright \b\f1\fs20\cgrid 
Returns\par}\pard\plain \s82\li720\widctlpar\ql\adjustright \fs20\cgrid {\s17 \sa60 \sb30
int The number of intput channels to the classifer class "classfier". \par
}}}}
{\xe \v load_weights\:MobileNetV2}
{\xe \v MobileNetV2\:load_weights}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
void MobileNetV2::load_weights (std::string  {\i pt}){\f2 [inline]}}}
\par
{\bkmkstart AAAAAAAAAU}
{\bkmkend AAAAAAAAAU}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
\par
{
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Loads a .pt weight file containing a dic with key/parameter pairs See {\f2 https://github.com/pytorch/pytorch/issues/36577}. }}\par
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
{\par
{\s5\sb90\sa30\keepn\widctlpar\adjustright \b\f1\fs20\cgrid 
Parameters\par}
\pard\plain \s81\li360\widctlpar\ql\adjustright \fs20\cgrid \trowd \trgaph108\trleft426\tblind426\trbrdrt\brdrs\brdrw10\brdrcf15 \trbrdrl\brdrs\brdrw10\brdrcf15 \trbrdrb\brdrs\brdrw10\brdrcf15 \trbrdrr\brdrs\brdrw10\brdrcf15 \trbrdrh\brdrs\brdrw10\brdrcf15 \trbrdrv\brdrs\brdrw10\brdrcf15 
\clvertalt\clbrdrt\brdrs\brdrw10\brdrcf15 \clbrdrl\brdrs\brdrw10\brdrcf15 \clbrdrb\brdrs\brdrw10\brdrcf15 \clbrdrr \brdrs\brdrw10\brdrcf15 \cltxlrtb \cellx2187
\clvertalt\clbrdrt\brdrs\brdrw10\brdrcf15 \clbrdrl\brdrs\brdrw10\brdrcf15 \clbrdrb\brdrs\brdrw10\brdrcf15 \clbrdrr \brdrs\brdrw10\brdrcf15 \cltxlrtb \cellx8748
\pard \widctlpar\intbl\adjustright
{{\i pt} \cell }{filename of the .pt weight file \cell }
{\row }
}
}}
{\xe \v preprocess\:MobileNetV2}
{\xe \v MobileNetV2\:preprocess}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
static torch::Tensor MobileNetV2::preprocess (cv::Mat  {\i img}, bool  {\i resizeOnly} = {\f2 false}){\f2 [inline]}, {\f2 [static]}}}
\par
{\bkmkstart AAAAAAAAAV}
{\bkmkend AAAAAAAAAV}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
\par
{
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Preprocessing of an openCV image for inference or learning The images are resized to 256x256, followed by a central crop of 224x224. }}\par
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Finally the values are first rescaled to [0.0, 1.0] and then normalized using mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225].\par
{{\s5\sb90\sa30\keepn\widctlpar\adjustright \b\f1\fs20\cgrid 
Parameters\par}
\pard\plain \s81\li360\widctlpar\ql\adjustright \fs20\cgrid \trowd \trgaph108\trleft426\tblind426\trbrdrt\brdrs\brdrw10\brdrcf15 \trbrdrl\brdrs\brdrw10\brdrcf15 \trbrdrb\brdrs\brdrw10\brdrcf15 \trbrdrr\brdrs\brdrw10\brdrcf15 \trbrdrh\brdrs\brdrw10\brdrcf15 \trbrdrv\brdrs\brdrw10\brdrcf15 
\clvertalt\clbrdrt\brdrs\brdrw10\brdrcf15 \clbrdrl\brdrs\brdrw10\brdrcf15 \clbrdrb\brdrs\brdrw10\brdrcf15 \clbrdrr \brdrs\brdrw10\brdrcf15 \cltxlrtb \cellx2187
\clvertalt\clbrdrt\brdrs\brdrw10\brdrcf15 \clbrdrl\brdrs\brdrw10\brdrcf15 \clbrdrb\brdrs\brdrw10\brdrcf15 \clbrdrr \brdrs\brdrw10\brdrcf15 \cltxlrtb \cellx8748
\pard \widctlpar\intbl\adjustright
{{\i img} \cell }{8bit BGR openCV image with an aspect ratio of 1:1 \cell }
{\row }
\trowd \trgaph108\trleft426\tblind426\trbrdrt\brdrs\brdrw10\brdrcf15 \trbrdrl\brdrs\brdrw10\brdrcf15 \trbrdrb\brdrs\brdrw10\brdrcf15 \trbrdrr\brdrs\brdrw10\brdrcf15 \trbrdrh\brdrs\brdrw10\brdrcf15 \trbrdrv\brdrs\brdrw10\brdrcf15 
\clvertalt\clbrdrt\brdrs\brdrw10\brdrcf15 \clbrdrl\brdrs\brdrw10\brdrcf15 \clbrdrb\brdrs\brdrw10\brdrcf15 \clbrdrr \brdrs\brdrw10\brdrcf15 \cltxlrtb \cellx2187
\clvertalt\clbrdrt\brdrs\brdrw10\brdrcf15 \clbrdrl\brdrs\brdrw10\brdrcf15 \clbrdrb\brdrs\brdrw10\brdrcf15 \clbrdrr \brdrs\brdrw10\brdrcf15 \cltxlrtb \cellx8748
\pard \widctlpar\intbl\adjustright
{{\i resizeOnly} \cell }{If true the image is only resized to 224x224 but not cropped. Default: false. \cell }
{\row }
}
{{\s5\sb90\sa30\keepn\widctlpar\adjustright \b\f1\fs20\cgrid 
Returns\par}\pard\plain \s82\li720\widctlpar\ql\adjustright \fs20\cgrid {\s17 \sa60 \sb30
torch::Tensor The image as a tensor ready to be used for inference and learning. \par
}}}}
{\pard\widctlpar\brdrb\brdrs\brdrw5\brsp20 \adjustright \par}
\pard\plain \s3\sb240\sa60\keepn\widctlpar\adjustright \b\f1\cgrid 
Member Data Documentation\par
\pard\plain 
{\xe \v classifierModuleName\:MobileNetV2}
{\xe \v MobileNetV2\:classifierModuleName}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
constexpr char MobileNetV2::classifierModuleName[] = "classifier"{\f2 [static]}, {\f2 [constexpr]}}}
\par
{\bkmkstart AAAAAAAAAW}
{\bkmkend AAAAAAAAAW}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
\par
{
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Name of the classifier module. }}\par
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
The name of the classifier module is needed whne replacing the default classifier. \par
}}
{\pard\widctlpar\brdrb\brdrs\brdrw5\brsp20 \adjustright \par}
The documentation for this class was generated from the following file:{\par
\pard\plain \s121\fi-360\li720\widctlpar\jclisttab\tx720{\*\pn \pnlvlbody\ilvl0\ls2\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
mobilenet_v2.h\par
}
\pard\plain \sect\sbkpage
\s1\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs36\kerning36\cgrid 
File Documentation{\tc \v File Documentation}
\par \pard\plain 
\pard\plain \s2\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs28\kerning28\cgrid 
mobilenet_v2.h\par \pard\plain 
{\bkmkstart AAAAAAAAAA}
{\bkmkend AAAAAAAAAA}
{
\pard\plain \s40\li0\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 1 {\cf21 #pragma once}\par
2 \par
3 {\cf21 #include <torch/torch.h>}\par
4 {\cf21 #include <vector>}\par
5 {\cf21 #include <algorithm>}\par
6 {\cf21 #include <fstream>}\par
7 {\cf21 #include <string>}\par
8 {\cf21 #include <regex>}\par
9 {\cf21 #include <filesystem>}\par
10 {\cf21 #include <iostream>}\par
11 {\cf21 #include <system_error>}\par
12 {\cf21 #include <opencv2/opencv.hpp>}\par
13 \par
14 {\cf20 /***}\par
15 {\cf20  * MobileNetV2 C++ Implementation (LibTorch)}\par
16 {\cf20  * Which is able to load the pre-trained weights from torchvision}\par
17 {\cf20  * (c) 2025 Bernd Porr, GPLv3}\par
18 {\cf20  ***/}\par
19 \par
20 {\cf21 #ifdef NDEBUG}\par
21 {\cf17 constexpr} {\cf18 bool} debugOutput = {\cf17 false};\par
22 {\cf21 #else}\par
23 {\cf17 constexpr} {\cf18 bool} debugOutput = {\cf17 true};\par
24 {\cf21 #endif}\par
25 \par
30 {\cf17 class }Conv2dNormActivation : {\cf17 public} torch::nn::Module\par
31 \{\par
32 {\cf17 public}:\par
33     {\cf17 static} {\cf17 constexpr} {\cf18 char} className[] = {\cf22 "Conv2dNormActivation"};\par
34 \par
35     {\cf17 static} {\cf17 inline} torch::Tensor relu6({\cf17 const} torch::Tensor &x)\par
36     \{\par
37         {\cf19 return} torch::clamp(torch::relu(x), 0, 6);\par
38     \}\par
39 \par
40     Conv2dNormActivation({\cf18 int} in_channels,\par
41                          {\cf18 int} out_channels,\par
42                          {\cf18 int} kernel_size = 3,\par
43                          {\cf18 int} stride = 1,\par
44                          {\cf18 int} padding = -1,\par
45                          {\cf18 int} groups = 1)\par
46     \{\par
47         {\cf17 const} {\cf18 int} dilation = 1;\par
48         conv = torch::nn::Sequential();\par
49         {\cf19 if} (padding < 0)\par
50         \{\par
51             padding = (kernel_size - 1) / 2 * dilation;\par
52         \}\par
53         conv->push_back(torch::nn::Conv2d(\par
54             torch::nn::Conv2dOptions(in_channels, out_channels, kernel_size)\par
55                 .stride(stride)\par
56                 .padding(padding)\par
57                 .groups(groups)\par
58                 .bias({\cf17 false})));\par
59         conv->push_back(torch::nn::BatchNorm2d(torch::nn::BatchNorm2dOptions(out_channels)));\par
60         conv->push_back(torch::nn::Functional(relu6));\par
61         register_module(className, conv);\par
62     \}\par
63 \par
64     torch::Tensor forward({\cf17 const} torch::Tensor &x)\par
65     \{\par
66         {\cf19 return} conv->forward(x);\par
67     \}\par
68 \par
69 {\cf17 private}:\par
70     torch::nn::Sequential conv\{{\cf17 nullptr}\};\par
71 \};\par
72 \par
77 {\cf17 class }InvertedResidual : {\cf17 public} torch::nn::Module\par
78 \{\par
79 {\cf17 public}:\par
80     {\cf17 static} {\cf17 constexpr} {\cf18 char} className[] = {\cf22 "InvertedResidual"};\par
81 \par
82     InvertedResidual({\cf18 int} inp, {\cf18 int} oup, {\cf18 int} stride, {\cf18 int} expand_ratio)\par
83     \{\par
84         {\cf19 if} ((stride < 1) || (stride > 2))\par
85         \{\par
86             {\cf19 throw} std::invalid_argument({\cf22 "Stride needs to be 1 or 2."});\par
87         \}\par
88         {\cf17 const} {\cf18 int} hidden_dim = (int)round(inp * expand_ratio);\par
89         use_res_connect = (stride == 1) && (inp == oup);\par
90 \par
91         conv = torch::nn::Sequential();\par
92 \par
93         {\cf19 if} (expand_ratio != 1)\par
94         \{\par
95             conv->push_back(\par
96                 Conv2dNormActivation(inp,\par
97                                      hidden_dim,\par
98                                      {\cf20 /*kernel_size*/} 1));\par
99         \}\par
100 \par
101         conv->push_back(\par
102             Conv2dNormActivation(hidden_dim,\par
103                                  hidden_dim,\par
104                                  {\cf20 /*kernel_size=*/}3,\par
105                                  {\cf20 /*stride=*/}stride,\par
106                                  {\cf20 /*padding=*/}-1,\par
107                                  {\cf20 /*groups=*/}hidden_dim));\par
108 \par
109         conv->push_back(torch::nn::Conv2d(\par
110             torch::nn::Conv2dOptions(hidden_dim, oup,\par
111                                      {\cf20 /*kernel_size=*/}1)\par
112                 .stride(1)\par
113                 .padding(0)\par
114                 .bias({\cf17 false})));\par
115         conv->push_back(torch::nn::BatchNorm2d(torch::nn::BatchNorm2dOptions(oup)));\par
116 \par
117         register_module(className, conv);\par
118     \}\par
119 \par
120     torch::Tensor forward({\cf17 const} torch::Tensor &x)\par
121     \{\par
122         {\cf19 if} (use_res_connect)\par
123         \{\par
124             {\cf19 return} x + conv->forward(x);\par
125         \}\par
126         {\cf19 else}\par
127         \{\par
128             {\cf19 return} conv->forward(x);\par
129         \}\par
130     \}\par
131     torch::nn::Sequential conv\{{\cf17 nullptr}\};\par
132     {\cf18 bool} use_res_connect;\par
133 \};\par
134 \par
139 {\cf17 class }MobileNetV2 : {\cf17 public} torch::nn::Module\par
140 \{\par
141 {\cf17 public}:\par
142     {\cf20 // Module name of the feature detector.}\par
143     {\cf17 static} {\cf17 constexpr} {\cf18 char} featuresModuleName[] = {\cf22 "features"};\par
144 \par
150     {\cf17 static} {\cf17 constexpr} {\cf18 char} classifierModuleName[] = {\cf22 "classifier"};\par
151 \par
162     MobileNetV2({\cf18 int} num_classes = 1000, {\cf18 float} width_mult = 1.0f, {\cf18 int} round_nearest = 8, {\cf18 float} dropout = 0.2)\par
163     \{\par
164         {\cf18 int} input_channels = 32;\par
165         input_channels = make_divisible(input_channels * width_mult, round_nearest);\par
166         features_output_channels = make_divisible(features_output_channels * std::max(1.0f, width_mult), round_nearest);\par
167 \par
168         features = torch::nn::Sequential();\par
169 \par
170         features->push_back(\par
171             Conv2dNormActivation(3,\par
172                                  input_channels,\par
173                                  {\cf20 /*kernel_size=*/}3,\par
174                                  {\cf20 /*stride =*/}2));\par
175 \par
176         {\cf20 // inverted residual blocks}\par
177         {\cf19 for} ({\cf17 const} {\cf17 auto} &cfg : inverted_residual_setting)\par
178         \{\par
179             {\cf17 const} {\cf18 int} t = cfg[0];\par
180             {\cf17 const} {\cf18 int} c = cfg[1];\par
181             {\cf17 const} {\cf18 int} n = cfg[2];\par
182             {\cf17 const} {\cf18 int} s = cfg[3];\par
183 \par
184             {\cf18 int} output_channel = make_divisible(c * width_mult, round_nearest);\par
185             {\cf19 for} ({\cf18 int} i = 0; i < n; ++i)\par
186             \{\par
187                 {\cf17 const} {\cf18 int} stride = (i == 0) ? s : 1;\par
188                 features->push_back(\par
189                     InvertedResidual(input_channels, output_channel, stride, t));\par
190                 input_channels = output_channel;\par
191             \}\par
192         \}\par
193 \par
194         features->push_back(\par
195             Conv2dNormActivation(input_channels,\par
196                                  features_output_channels,\par
197                                  {\cf20 /*kernel_size=*/}1));\par
198 \par
199         register_module(featuresModuleName, features);\par
200 \par
201         {\cf20 // classifier: Dropout + Linear}\par
202         classifier = torch::nn::Sequential();\par
203         classifier->push_back(torch::nn::Dropout(torch::nn::DropoutOptions(dropout)));\par
204         classifier->push_back(torch::nn::Linear(torch::nn::LinearOptions(features_output_channels, num_classes)));\par
205         register_module(classifierModuleName, classifier);\par
206     \}\par
207 \par
215     {\cf18 int} getNinputChannels4Classifier(){\cf17  const}\par
216 {\cf17     }\{\par
217         {\cf19 return} features_output_channels;\par
218     \}\par
219 \par
226     torch::Tensor forward(torch::Tensor x)\par
227     \{\par
228         x = features->forward(x);\par
229         {\cf17 const} torch::nn::functional::AdaptiveAvgPool2dFuncOptions &ar = torch::nn::functional::AdaptiveAvgPool2dFuncOptions(\{1, 1\});\par
230         x = torch::nn::functional::adaptive_avg_pool2d(x, ar);\par
231         x = torch::flatten(x, 1);\par
232         x = classifier->forward(x);\par
233         {\cf19 return} x;\par
234     \}\par
235 \par
239     {\cf18 void} initialize_weights()\par
240     \{\par
241         {\cf19 for} ({\cf17 auto} &module : modules({\cf20 /*include_self=*/}{\cf17 false}))\par
242         \{\par
243             {\cf19 if} ({\cf17 auto} M = {\cf17 dynamic_cast<}torch::nn::Conv2dImpl *{\cf17 >}(module.get()))\par
244             \{\par
245                 torch::nn::init::kaiming_normal_(M->weight, {\cf20 /*a=*/}0, torch::kFanOut, torch::kReLU);\par
246                 {\cf19 if} (M->options.bias())\par
247                     torch::nn::init::zeros_(M->bias);\par
248             \}\par
249             {\cf19 else} {\cf19 if} ({\cf17 auto} M = {\cf17 dynamic_cast<}torch::nn::BatchNorm2dImpl *{\cf17 >}(module.get()))\par
250             \{\par
251                 torch::nn::init::ones_(M->weight);\par
252                 torch::nn::init::zeros_(M->bias);\par
253             \}\par
254             {\cf19 else} {\cf19 if} ({\cf17 auto} M = {\cf17 dynamic_cast<}torch::nn::LinearImpl *{\cf17 >}(module.get()))\par
255             \{\par
256                 torch::nn::init::normal_(M->weight, 0.0, 0.01);\par
257                 torch::nn::init::zeros_(M->bias);\par
258             \}\par
259         \}\par
260     \}\par
261 \par
268     {\cf18 void} load_weights(std::string pt)\par
269     \{\par
270         std::ifstream input(pt, std::ios::binary);\par
271         input.exceptions(input.failbit);\par
272         std::vector<char> bytes(\par
273             (std::istreambuf_iterator<char>(input)),\par
274             (std::istreambuf_iterator<char>()));\par
275         input.close();\par
276         {\cf17 const} c10::Dict<c10::IValue, c10::IValue> weights = torch::pickle_load(bytes).toGenericDict();\par
277         {\cf19 if} (debugOutput)\par
278         \{\par
279             std::cerr << {\cf22 "Parameters we have in this model here: "} << std::endl;\par
280             {\cf19 for} ({\cf17 auto} {\cf17 const} &m : named_parameters())\par
281             \{\par
282                 {\cf17 auto} k = ourkey2torchvision(m.key());\par
283                 std::cerr << m.key() << {\cf22 "->"} << k << {\cf22 ": "} << m.value().sizes() << std::endl;\par
284             \}\par
285             std::cerr << {\cf22 "Named buffers we have in this model here: "} << std::endl;\par
286             {\cf19 for} ({\cf17 const} {\cf17 auto} &b : named_buffers())\par
287             \{\par
288                 {\cf17 auto} k = ourkey2torchvision(b.key());\par
289                 std::cout << b.key() << {\cf22 "->"} << k << {\cf22 ": "} << b.value().sizes() << std::endl;\par
290             \}\par
291             std::cerr << {\cf22 "Parameters we have in the weight file "} << pt << {\cf22 ":"} << std::endl;\par
292             {\cf19 for} ({\cf17 auto} {\cf17 const} &w : weights)\par
293             \{\par
294                 std::cerr << w.key() << {\cf22 ": "} << w.value().toTensor().sizes() << std::endl;\par
295             \}\par
296         \}\par
297         torch::NoGradGuard no_grad;\par
298         {\cf19 if} (debugOutput)\par
299             std::cerr << {\cf22 "Loading weights"} << std::endl;\par
300         {\cf19 for} ({\cf17 auto} &m : named_parameters())\par
301         \{\par
302             {\cf17 const} std::string model_key = m.key();\par
303             {\cf17 const} std::string model_key4torchvision = ourkey2torchvision(model_key);\par
304             {\cf19 if} (debugOutput)\par
305                 std::cerr << {\cf22 "Searching for: "} << model_key4torchvision << {\cf22 ": "} << m.value().sizes() << std::endl;\par
306             {\cf18 bool} foundit = {\cf17 false};\par
307             {\cf19 for} ({\cf17 auto} {\cf17 const} &w : weights)\par
308             \{\par
309                 {\cf19 if} (model_key4torchvision == w.key())\par
310                 \{\par
311                     {\cf19 if} (debugOutput)\par
312                         std::cerr << {\cf22 "Found it: "} << w.key() << std::endl;\par
313                     m.value().copy_(w.value().toTensor());\par
314                     foundit = {\cf17 true};\par
315                     {\cf19 break};\par
316                 \}\par
317             \}\par
318             {\cf19 if} (!foundit)\par
319                 std::cerr << {\cf22 "Key: "} << model_key4torchvision << {\cf22 " could not be found!"} << std::endl;\par
320         \}\par
321         {\cf19 if} (debugOutput)\par
322             std::cerr << {\cf22 "Loading named buffers"} << std::endl;\par
323         {\cf19 for} ({\cf17 auto} &b : named_buffers())\par
324         \{\par
325             std::string model_key = b.key();\par
326             std::string model_key4torchvision = ourkey2torchvision(model_key);\par
327             {\cf19 if} (debugOutput)\par
328                 std::cerr << {\cf22 "Searching for: "} << model_key4torchvision << {\cf22 ": "} << b.value().sizes() << std::endl;\par
329             {\cf18 bool} foundit = {\cf17 false};\par
330             {\cf19 for} ({\cf17 auto} {\cf17 const} &w : weights)\par
331             \{\par
332                 {\cf19 if} (model_key4torchvision == w.key())\par
333                 \{\par
334                     {\cf19 if} (debugOutput)\par
335                         std::cerr << {\cf22 "Found it: "} << w.key() << std::endl;\par
336                     b.value().copy_(w.value().toTensor());\par
337                     foundit = {\cf17 true};\par
338                     {\cf19 break};\par
339                 \}\par
340             \}\par
341             {\cf19 if} (!foundit)\par
342                 std::cerr << {\cf22 "Key: "} << model_key4torchvision << {\cf22 " could not be found!"} << std::endl;\par
343         \}\par
344     \}\par
345 \par
356     {\cf17 static} torch::Tensor preprocess(cv::Mat img, {\cf18 bool} resizeOnly = {\cf17 false})\par
357     \{\par
358         {\cf17 constexpr} {\cf18 int} imageSizeBeforeCrop = 256;\par
359         {\cf17 constexpr} {\cf18 int} finalImageSize = 224;\par
360         {\cf17 constexpr} {\cf18 int} numChannels = 3; {\cf20 // colour}\par
361 \par
362         {\cf19 if} (img.depth() != CV_8U)\par
363             {\cf19 throw} std::invalid_argument({\cf22 "Image is not 8bit."});\par
364         {\cf19 if} (img.channels() != numChannels)\par
365             {\cf19 throw} std::invalid_argument({\cf22 "Image is not BGR / colour."});\par
366 \par
367         {\cf19 if} (resizeOnly)\par
368         \{\par
369             cv::resize(img, img, cv::Size(finalImageSize, finalImageSize));\par
370         \}\par
371         {\cf19 else}\par
372         \{\par
373             cv::resize(img, img, cv::Size(imageSizeBeforeCrop, imageSizeBeforeCrop));\par
374             {\cf17 constexpr} {\cf18 int} start = (imageSizeBeforeCrop - finalImageSize) / 2;\par
375             {\cf17 const} cv::Rect roi(start, start, finalImageSize, finalImageSize);\par
376             img = img(roi).clone();\par
377         \}\par
378         cv::cvtColor(img, img, cv::COLOR_BGR2RGB);\par
379 \par
380         torch::Tensor tensor = torch::from_blob(img.data, \{img.rows, img.cols, 3\}, torch::kByte);\par
381         tensor = tensor.permute(\{2, 0, 1\}).to(torch::kFloat).div_(255.0);\par
382         tensor = torch::data::transforms::Normalize(\{0.485, 0.456, 0.406\}, \{0.229, 0.224, 0.225\})(tensor);\par
383         {\cf19 return} tensor;\par
384     \}\par
385 \par
390     torch::nn::Sequential classifier\{{\cf17 nullptr}\}; {\cf20 // Dropout + Linear}\par
391 \par
396     torch::nn::Sequential features\{{\cf17 nullptr}\};\par
397 \par
398     {\cf17 private}:\par
399 \par
400     {\cf20 // helper which maps the libtorch keys to pytorch keys}\par
401     std::string ourkey2torchvision(std::string k){\cf17  const}\par
402 {\cf17     }\{\par
403         {\cf20 // called simply "conv" in the weights file}\par
404         k = std::regex_replace(k, std::regex(InvertedResidual::className), {\cf22 "conv"});\par
405         {\cf20 // not used at all in the weights file}\par
406         {\cf17 const} std::string r = std::string(Conv2dNormActivation::className) + {\cf22 "\\\\."};\par
407         k = std::regex_replace(k, std::regex(r), {\cf22 ""});\par
408         {\cf19 return} k;\par
409     \}\par
410 \par
411     {\cf20 // makes a value divisible}\par
412     {\cf17 inline} {\cf18 int} make_divisible({\cf18 int} v, {\cf18 int} divisor = 8, {\cf18 int} min_value = -1){\cf17  const}\par
413 {\cf17     }\{\par
414         {\cf19 if} (min_value < 0)\par
415             min_value = divisor;\par
416         {\cf18 int} new_v = std::max(min_value, (({\cf18 int})((({\cf18 int})(v + divisor / 2)) / divisor)) * divisor);\par
417         {\cf19 if} (new_v < (0.9 * ({\cf18 float})v))\par
418             new_v += divisor;\par
419         {\cf19 return} new_v;\par
420     \}\par
421 \par
422     {\cf20 // features output channels but can be scaled}\par
423     {\cf18 int} features_output_channels = 1280;\par
424 \par
425     {\cf20 // MobileNetV2 inverted residual settings:}\par
426     {\cf20 // t, c, n, s  (expansion, output channels, repeats, stride)}\par
427     {\cf17 const} std::vector<std::array<int, 4>> inverted_residual_setting = \{\par
428         \{1, 16, 1, 1\},\par
429         \{6, 24, 2, 2\},\par
430         \{6, 32, 3, 2\},\par
431         \{6, 64, 4, 2\},\par
432         \{6, 96, 3, 1\},\par
433         \{6, 160, 3, 2\},\par
434         \{6, 320, 1, 1\},\par
435     \};\par
436 \};\par
}

\pard\plain \sect\sbkpage
\s1\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs36\kerning36\cgrid 
\s1\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs36\kerning36\cgrid Index\par 
\pard\plain 
{\tc \v Index}
{\field\fldedit {\*\fldinst INDEX \\c2 \\*MERGEFORMAT}{\fldrslt INDEX}}
}
