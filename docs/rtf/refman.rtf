{\rtf1\ansi\ansicpg1252\uc1 \deff0\deflang1033\deflangfe1033
{\fonttbl {\f0\froman\fcharset0\fprq2{\*\panose 02020603050405020304}Times New Roman;}
{\f1\fswiss\fcharset0\fprq2{\*\panose 020b0604020202020204}Arial;}
{\f2\fmodern\fcharset0\fprq1{\*\panose 02070309020205020404}Courier New;}
{\f3\froman\fcharset2\fprq2{\*\panose 05050102010706020507}Symbol;}
}
{\colortbl;\red0\green0\blue0;\red0\green0\blue255;\red0\green255\blue255;\red0\green255\blue0;\red255\green0\blue255;\red255\green0\blue0;\red255\green255\blue0;\red255\green255\blue255;\red0\green0\blue128;\red0\green128\blue128;\red0\green128\blue0;\red128\green0\blue128;\red128\green0\blue0;\red128\green128\blue0;\red128\green128\blue128;\red192\green192\blue192;\red0\green128\blue0;\red96\green64\blue32;\rede0\green128\blue0;\red128\green0\blue0;\red128\green96\blue32;\red0\green32\blue128;\red0\green128\blue128;\red255\green0\blue255;\red0\green0\blue0;\red112\green0\blue112;\red255\green0\blue0;}
{\stylesheet
{\widctlpar\adjustright \fs20\cgrid \snext0 Normal;}
{\paperw11900\paperh16840\margl1800\margr1800\margt1440\margb1440\gutter0\ltrsect}
{\s1\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs36\kerning36\cgrid \sbasedon0 \snext0 heading 1;}
{\s2\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs28\kerning28\cgrid \sbasedon0 \snext0 heading 2;}
{\s3\sb240\sa60\keepn\widctlpar\adjustright \b\f1\cgrid \sbasedon0 \snext0 heading 3;}
{\s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid \sbasedon0 \snext0 heading 4;}{\*\cs10 \additive Default Paragraph Font;}
{\s5\sb90\sa30\keepn\widctlpar\adjustright \b\f1\fs20\cgrid \sbasedon0 \snext0 heading 5;}{\*\cs10 \additive Default Paragraph Font;}
{\s15\qc\sb240\sa60\widctlpar\outlinelevel0\adjustright \b\f1\fs32\kerning28\cgrid \sbasedon0 \snext15 Title;}
{\s16\qc\sa60\widctlpar\outlinelevel1\adjustright \f1\cgrid \sbasedon0 \snext16 Subtitle;}
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid \sbasedon0 \snext17 BodyText;}
{\s18\widctlpar\fs22\cgrid \sbasedon0 \snext18 DenseText;}
{\s28\widctlpar\tqc\tx4320\tqr\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext28 header;}
{\s29\widctlpar\tqc\tx4320\tqr\tx8640\qr\adjustright \fs20\cgrid \sbasedon0 \snext29 footer;}
{\s30\li360\sa60\sb120\keepn\widctlpar\adjustright \b\f1\fs20\cgrid \sbasedon0 \snext30 GroupHeader;}
{\s40\li0\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext41 Code Example 0;}
{\s41\li360\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext42 Code Example 1;}
{\s42\li720\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext43 Code Example 2;}
{\s43\li1080\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext44 Code Example 3;}
{\s44\li1440\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext45 Code Example 4;}
{\s45\li1800\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext46 Code Example 5;}
{\s46\li2160\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext47 Code Example 6;}
{\s47\li2520\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext48 Code Example 7;}
{\s48\li2880\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext49 Code Example 8;}
{\s49\li3240\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext50 Code Example 9;}
{\s50\li3600\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext51 Code Example 10;}
{\s51\li3960\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext52 Code Example 11;}
{\s52\li4320\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext53 Code Example 12;}
{\s53\li4680\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid \sbasedon0 \snext53 Code Example 13;}
{\s60\li0\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext61 List Continue 0;}
{\s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext62 List Continue 1;}
{\s62\li720\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext63 List Continue 2;}
{\s63\li1080\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext64 List Continue 3;}
{\s64\li1440\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext65 List Continue 4;}
{\s65\li1800\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext66 List Continue 5;}
{\s66\li2160\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext67 List Continue 6;}
{\s67\li2520\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext68 List Continue 7;}
{\s68\li2880\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext69 List Continue 8;}
{\s69\li3240\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext70 List Continue 9;}
{\s70\li3600\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext71 List Continue 10;}
{\s71\li3960\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext72 List Continue 11;}
{\s72\li4320\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext73 List Continue 12;}
{\s73\li4680\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid \sbasedon0 \snext73 List Continue 13;}
{\s80\li0\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext81 DescContinue 0;}
{\s81\li360\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext82 DescContinue 1;}
{\s82\li720\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext83 DescContinue 2;}
{\s83\li1080\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext84 DescContinue 3;}
{\s84\li1440\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext85 DescContinue 4;}
{\s85\li1800\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext86 DescContinue 5;}
{\s86\li2160\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext87 DescContinue 6;}
{\s87\li2520\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext88 DescContinue 7;}
{\s88\li2880\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext89 DescContinue 8;}
{\s89\li3240\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext90 DescContinue 9;}
{\s90\li3600\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext91 DescContinue 10;}
{\s91\li3960\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext92 DescContinue 11;}
{\s92\li4320\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext93 DescContinue 12;}
{\s93\li4680\widctlpar\ql\adjustright \fs20\cgrid \sbasedon0 \snext93 DescContinue 13;}
{\s100\li0\sa30\sb30\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext101 LatexTOC 0;}
{\s101\li360\sa27\sb27\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext102 LatexTOC 1;}
{\s102\li720\sa24\sb24\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext103 LatexTOC 2;}
{\s103\li1080\sa21\sb21\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext104 LatexTOC 3;}
{\s104\li1440\sa18\sb18\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext105 LatexTOC 4;}
{\s105\li1800\sa15\sb15\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext106 LatexTOC 5;}
{\s106\li2160\sa12\sb12\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext107 LatexTOC 6;}
{\s107\li2520\sa9\sb9\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext108 LatexTOC 7;}
{\s108\li2880\sa6\sb6\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext109 LatexTOC 8;}
{\s109\li3240\sa3\sb3\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext110 LatexTOC 9;}
{\s110\li3600\sa3\sb3\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext111 LatexTOC 10;}
{\s111\li3960\sa3\sb3\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext112 LatexTOC 11;}
{\s112\li4320\sa3\sb3\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext113 LatexTOC 12;}
{\s113\li4680\sa3\sb3\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid \sbasedon0 \snext113 LatexTOC 13;}
{\s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext121 \sautoupd List Bullet 0;}
{\s121\fi-360\li720\widctlpar\jclisttab\tx720{\*\pn \pnlvlbody\ilvl0\ls2\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext122 \sautoupd List Bullet 1;}
{\s122\fi-360\li1080\widctlpar\jclisttab\tx1080{\*\pn \pnlvlbody\ilvl0\ls3\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext123 \sautoupd List Bullet 2;}
{\s123\fi-360\li1440\widctlpar\jclisttab\tx1440{\*\pn \pnlvlbody\ilvl0\ls4\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext124 \sautoupd List Bullet 3;}
{\s124\fi-360\li1800\widctlpar\jclisttab\tx1800{\*\pn \pnlvlbody\ilvl0\ls5\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext125 \sautoupd List Bullet 4;}
{\s125\fi-360\li2160\widctlpar\jclisttab\tx2160{\*\pn \pnlvlbody\ilvl0\ls6\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext126 \sautoupd List Bullet 5;}
{\s126\fi-360\li2520\widctlpar\jclisttab\tx2520{\*\pn \pnlvlbody\ilvl0\ls7\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext127 \sautoupd List Bullet 6;}
{\s127\fi-360\li2880\widctlpar\jclisttab\tx2880{\*\pn \pnlvlbody\ilvl0\ls8\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext128 \sautoupd List Bullet 7;}
{\s128\fi-360\li3240\widctlpar\jclisttab\tx3240{\*\pn \pnlvlbody\ilvl0\ls9\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext129 \sautoupd List Bullet 8;}
{\s129\fi-360\li3600\widctlpar\jclisttab\tx3600{\*\pn \pnlvlbody\ilvl0\ls10\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext130 \sautoupd List Bullet 9;}
{\s130\fi-360\li3960\widctlpar\jclisttab\tx3960{\*\pn \pnlvlbody\ilvl0\ls11\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext131 \sautoupd List Bullet 10;}
{\s131\fi-360\li4320\widctlpar\jclisttab\tx4320{\*\pn \pnlvlbody\ilvl0\ls12\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext132 \sautoupd List Bullet 11;}
{\s132\fi-360\li4680\widctlpar\jclisttab\tx4680{\*\pn \pnlvlbody\ilvl0\ls13\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext133 \sautoupd List Bullet 12;}
{\s133\fi-360\li5040\widctlpar\jclisttab\tx5040{\*\pn \pnlvlbody\ilvl0\ls14\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid \sbasedon0 \snext133 \sautoupd List Bullet 13;}
{\s140\fi-360\li360\widctlpar\fs20\cgrid \sbasedon0 \snext141 \sautoupd List Enum 0;}
{\s141\fi-360\li720\widctlpar\fs20\cgrid \sbasedon0 \snext142 \sautoupd List Enum 1;}
{\s142\fi-360\li1080\widctlpar\fs20\cgrid \sbasedon0 \snext143 \sautoupd List Enum 2;}
{\s143\fi-360\li1440\widctlpar\fs20\cgrid \sbasedon0 \snext144 \sautoupd List Enum 3;}
{\s144\fi-360\li1800\widctlpar\fs20\cgrid \sbasedon0 \snext145 \sautoupd List Enum 4;}
{\s145\fi-360\li2160\widctlpar\fs20\cgrid \sbasedon0 \snext146 \sautoupd List Enum 5;}
{\s146\fi-360\li2520\widctlpar\fs20\cgrid \sbasedon0 \snext147 \sautoupd List Enum 6;}
{\s147\fi-360\li2880\widctlpar\fs20\cgrid \sbasedon0 \snext148 \sautoupd List Enum 7;}
{\s148\fi-360\li3240\widctlpar\fs20\cgrid \sbasedon0 \snext149 \sautoupd List Enum 8;}
{\s149\fi-360\li3600\widctlpar\fs20\cgrid \sbasedon0 \snext150 \sautoupd List Enum 9;}
{\s150\fi-360\li3960\widctlpar\fs20\cgrid \sbasedon0 \snext151 \sautoupd List Enum 10;}
{\s151\fi-360\li4320\widctlpar\fs20\cgrid \sbasedon0 \snext152 \sautoupd List Enum 11;}
{\s152\fi-360\li4680\widctlpar\fs20\cgrid \sbasedon0 \snext153 \sautoupd List Enum 12;}
{\s153\fi-360\li5040\widctlpar\fs20\cgrid \sbasedon0 \snext153 \sautoupd List Enum 13;}
}
{\info 
{\title {\comment Mobilenet with Libtorch }Mobilenet with Libtorch}
{\comment Generated by doxygen 1.9.8.}
}\pard\plain 
\sectd\pgnlcrm
{\footer \s29\widctlpar\tqc\tx4320\tqr\tx8640\qr\adjustright \fs20\cgrid {\chpgn}}
\pard\plain \s16\qc\sa60\widctlpar\outlinelevel1\adjustright \f1\cgrid 
\vertalc\qc\par\par\par\par\par\par\par
\pard\plain \s15\qc\sb240\sa60\widctlpar\outlinelevel0\adjustright \b\f1\fs32\kerning28\cgrid 
{\field\fldedit {\*\fldinst TITLE \\*MERGEFORMAT}{\fldrslt Mobilenet with Libtorch}}\par
\pard\plain \s16\qc\sa60\widctlpar\outlinelevel1\adjustright \f1\cgrid 
\par
\par\par\par\par\par\par\par\par\par\par\par\par
\pard\plain \s16\qc\sa60\widctlpar\outlinelevel1\adjustright \f1\cgrid 
{\field\fldedit {\*\fldinst AUTHOR \\*MERGEFORMAT}{\fldrslt AUTHOR}}\par
Version \par\page\page\vertalt
\pard\plain 
\s1\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs36\kerning36\cgrid Table of Contents\par
\pard\plain \par
{\field\fldedit {\*\fldinst TOC \\f \\*MERGEFORMAT}{\fldrslt Table of contents}}\par
\pard\plain 
\sect \sbkpage \pgndec \pgnrestart
\sect \sectd \sbknone
{\footer \s29\widctlpar\tqc\tx4320\tqr\tx8640\qr\adjustright \fs20\cgrid {\chpgn}}

\pard\plain \sect\sbkpage
\s1\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs36\kerning36\cgrid 
\par \pard\plain 
\pard\plain \s1\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs36\kerning36\cgrid 
MobileNetV2 libtorch\par \pard\plain 
{\tc\tcl1 \v MobileNetV2 libtorch}
{\xe \v MobileNetV2 libtorch}
{\bkmkstart AAAAAAAAAM}
{\bkmkend AAAAAAAAAM}
{
\pard\plain \s17\sa60\sb30\widctlpar\qj \fs22\cgrid {\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
{\bkmkstart AAAAAAAAAN}
{\bkmkend AAAAAAAAAN}
C++ version of {\b MobileNetV2} using libtorch which can import the pre-trained weights from torchvision.\par
Importing the weights instead of using the JIT has the advantage that one can do transfer learning also on an edge device, for example, to adapt to different situations locally.\par
This implementation follows very closely the {\f2 torchvision implementation of mobilenet v2}.\par
Mobilenet is described {\f2 here}.\par
Then simply include {\f2 {\b mobilenet_v2.h}}  into your own project.\par
{\pard\plain \s1\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs36\kerning36\cgrid {\tc\tcl1 Credit} \par}
(C) 2025 {\f2 Bernd Porr}, GPLv3 \par
}}

\pard\plain \sect\sbkpage
\s1\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs36\kerning36\cgrid 
Hierarchical Index\par \pard\plain 
{\tc \v Hierarchical Index}
\pard\plain \s2\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs28\kerning28\cgrid 
Class Hierarchy\par \pard\plain 
{
\pard\plain \s17\sa60\sb30\widctlpar\qj \fs22\cgrid This inheritance list is sorted roughly, but not completely, alphabetically:}
{
\par
\pard\plain \s101\li360\sa27\sb27\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid 
torch::nn::Module
{
\par
\pard\plain \s102\li720\sa24\sb24\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid 
MobileNetV2\tab {\field\fldedit {\*\fldinst PAGEREF AAAAAAAAAB \\*MERGEFORMAT}{\fldrslt pagenum}}
\par
}\par
}
\pard\plain \sect\sbkpage
\s1\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs36\kerning36\cgrid 
Class Index\par \pard\plain 
{\tc \v Class Index}
\pard\plain \s2\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs28\kerning28\cgrid 
Class List\par \pard\plain 
{
\pard\plain \s17\sa60\sb30\widctlpar\qj \fs22\cgrid Here are the classes, structs, unions and interfaces with brief descriptions:}
{
\par
\pard\plain \s101\li360\sa27\sb27\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid 
{\b {\b MobileNetV2} ({\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Implementation of {\b MobileNetV2} as done in py-torchvision See: // {\f2 https://github.com/pytorch/vision/blob/main/torchvision/models/mobilenetv2.py#L66} })} \tab {\field\fldedit {\*\fldinst PAGEREF AAAAAAAAAB \\*MERGEFORMAT}{\fldrslt pagenum}}
\par
\par}
\pard\plain \sect\sbkpage
\s1\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs36\kerning36\cgrid 
File Index\par \pard\plain 
{\tc \v File Index}
\pard\plain \s2\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs28\kerning28\cgrid 
File List\par \pard\plain 
{
\pard\plain \s17\sa60\sb30\widctlpar\qj \fs22\cgrid Here is a list of all documented files with brief descriptions:}
{
\par
\pard\plain \s101\li360\sa27\sb27\widctlpar\tqr\tldot\tx8640\adjustright \fs20\cgrid 
{\b {\b mobilenet_v2.h} } \tab {\field\fldedit {\*\fldinst PAGEREF AAAAAAAAAA \\*MERGEFORMAT}{\fldrslt pagenum}}
\par
\par}
\pard\plain \sect\sbkpage
\s1\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs36\kerning36\cgrid 
Class Documentation{\tc \v Class Documentation}
\par \pard\plain 
\pard\plain \s2\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs28\kerning28\cgrid 
MobileNetV2 Class Reference\par \pard\plain 
{\tc\tcl2 \v MobileNetV2}
{\xe \v MobileNetV2}
{\bkmkstart AAAAAAAAAB}
{\bkmkend AAAAAAAAAB}
\par
{
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Implementation of {\b MobileNetV2} as done in py-torchvision See: // {\f2 https://github.com/pytorch/vision/blob/main/torchvision/models/mobilenetv2.py#L66}. }}\par
{
{\f2 #include <mobilenet_v2.h>}}\par
Inheritance diagram for MobileNetV2:{
\pard\plain 
\par\pard \qc {\field\flddirty {\*\fldinst INCLUDEPICTURE "classMobileNetV2__inherit__graph.png" \\d \\*MERGEFORMAT}{\fldrslt IMAGE}}\par
}
\par
Collaboration diagram for MobileNetV2:{
\pard\plain 
\par\pard \qc {\field\flddirty {\*\fldinst INCLUDEPICTURE "classMobileNetV2__coll__graph.png" \\d \\*MERGEFORMAT}{\fldrslt IMAGE}}\par
}
\par
\pard\plain \s3\sb240\sa60\keepn\widctlpar\adjustright \b\f1\cgrid 
Public Member Functions\par
\pard\plain 

{
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
{\b MobileNetV2} (int num_classes=1000, float width_mult=1.0f, int round_nearest=8, float dropout=0.2)\par
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid {\i {\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Construct a new {\b MobileNetV2} object. }{
}\par
}\par}
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
int {\b getNinputChannels4Classifier} () const\par
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid {\i {\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Gets the number of input channels for the classfier. }{
}\par
}\par}
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
torch::Tensor {\b forward} (torch::Tensor x)\par
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid {\i {\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Performs the forward pass. }{
}\par
}\par}
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

void {\b initialize_weights} (){\bkmkstart AAAAAAAAAC}
{\bkmkend AAAAAAAAAC}
\par
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid {\i {\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Initialize conv/bn/linear similar to torchvision defaults. }{
}\par
}\par}
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
void {\b load_weights} (std::string pt)\par
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid {\i {\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Loads a .pt weight file containing a dict with key/parameter pairs. }{
}\par
}\par}
}
\pard\plain \s3\sb240\sa60\keepn\widctlpar\adjustright \b\f1\cgrid 
Static Public Member Functions\par
\pard\plain 

{
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
static torch::Tensor {\b preprocess} (cv::Mat img, bool resizeOnly=false)\par
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid {\i {\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Preprocessing of an openCV image for inference or learning. }{
}\par
}\par}
}
\pard\plain \s3\sb240\sa60\keepn\widctlpar\adjustright \b\f1\cgrid 
Public Attributes\par
\pard\plain 

{
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
torch::nn::Sequential {\b classifier}\par
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid {\i {\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Classifier submodule. }{
}\par
}\par}
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
torch::nn::Sequential {\b features}\par
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid {\i {\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Features submodule. }{
}\par
}\par}
}
\pard\plain \s3\sb240\sa60\keepn\widctlpar\adjustright \b\f1\cgrid 
Static Public Attributes\par
\pard\plain 

{
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 

static constexpr char {\b featuresModuleName} [] = "features"{\bkmkstart AAAAAAAAAD}
{\bkmkend AAAAAAAAAD}
\par
\pard\plain \s120\fi-360\li360\widctlpar\jclisttab\tx360{\*\pn \pnlvlbody\ilvl0\ls1\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
static constexpr char {\b classifierModuleName} [] = "classifier"\par
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid {\i {\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Name of the classifier module. }{
}\par
}\par}
}
{\pard\widctlpar\brdrb\brdrs\brdrw5\brsp20 \adjustright \par}
\pard\plain \s3\sb240\sa60\keepn\widctlpar\adjustright \b\f1\cgrid 
Detailed Description\par
\pard\plain 
{
\pard\plain \s17\sa60\sb30\widctlpar\qj \fs22\cgrid {\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Implementation of {\b MobileNetV2} as done in py-torchvision See: // {\f2 https://github.com/pytorch/vision/blob/main/torchvision/models/mobilenetv2.py#L66}. \par
}}
{\pard\widctlpar\brdrb\brdrs\brdrw5\brsp20 \adjustright \par}
\pard\plain \s3\sb240\sa60\keepn\widctlpar\adjustright \b\f1\cgrid 
Constructor & Destructor Documentation\par
\pard\plain 
{\xe \v MobileNetV2\:MobileNetV2}
{\xe \v MobileNetV2\:MobileNetV2}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
MobileNetV2::MobileNetV2 (int  {\i num_classes} = {\f2 1000}, float  {\i width_mult} = {\f2 1.0f}, int  {\i round_nearest} = {\f2 8}, float  {\i dropout} = {\f2 0.2}){\f2 [inline]}}}
\par
{\bkmkstart AAAAAAAAAE}
{\bkmkend AAAAAAAAAE}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
\par
{
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Construct a new {\b MobileNetV2} object. }}\par
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
If you want to load the weights from torchvision into the classifier use the default values for the parameters.\par
{{\s5\sb90\sa30\keepn\widctlpar\adjustright \b\f1\fs20\cgrid 
Parameters\par}
\pard\plain \s81\li360\widctlpar\ql\adjustright \fs20\cgrid \trowd \trgaph108\trleft426\tblind426\trbrdrt\brdrs\brdrw10\brdrcf15 \trbrdrl\brdrs\brdrw10\brdrcf15 \trbrdrb\brdrs\brdrw10\brdrcf15 \trbrdrr\brdrs\brdrw10\brdrcf15 \trbrdrh\brdrs\brdrw10\brdrcf15 \trbrdrv\brdrs\brdrw10\brdrcf15 
\clvertalt\clbrdrt\brdrs\brdrw10\brdrcf15 \clbrdrl\brdrs\brdrw10\brdrcf15 \clbrdrb\brdrs\brdrw10\brdrcf15 \clbrdrr \brdrs\brdrw10\brdrcf15 \cltxlrtb \cellx2187
\clvertalt\clbrdrt\brdrs\brdrw10\brdrcf15 \clbrdrl\brdrs\brdrw10\brdrcf15 \clbrdrb\brdrs\brdrw10\brdrcf15 \clbrdrr \brdrs\brdrw10\brdrcf15 \cltxlrtb \cellx8748
\pard \widctlpar\intbl\adjustright
{{\i num_classes} \cell }{Number of classes. \cell }
{\row }
\trowd \trgaph108\trleft426\tblind426\trbrdrt\brdrs\brdrw10\brdrcf15 \trbrdrl\brdrs\brdrw10\brdrcf15 \trbrdrb\brdrs\brdrw10\brdrcf15 \trbrdrr\brdrs\brdrw10\brdrcf15 \trbrdrh\brdrs\brdrw10\brdrcf15 \trbrdrv\brdrs\brdrw10\brdrcf15 
\clvertalt\clbrdrt\brdrs\brdrw10\brdrcf15 \clbrdrl\brdrs\brdrw10\brdrcf15 \clbrdrb\brdrs\brdrw10\brdrcf15 \clbrdrr \brdrs\brdrw10\brdrcf15 \cltxlrtb \cellx2187
\clvertalt\clbrdrt\brdrs\brdrw10\brdrcf15 \clbrdrl\brdrs\brdrw10\brdrcf15 \clbrdrb\brdrs\brdrw10\brdrcf15 \clbrdrr \brdrs\brdrw10\brdrcf15 \cltxlrtb \cellx8748
\pard \widctlpar\intbl\adjustright
{{\i width_mult} \cell }{Width multiplier - adjusts number of channels in each layer by this amount. \cell }
{\row }
\trowd \trgaph108\trleft426\tblind426\trbrdrt\brdrs\brdrw10\brdrcf15 \trbrdrl\brdrs\brdrw10\brdrcf15 \trbrdrb\brdrs\brdrw10\brdrcf15 \trbrdrr\brdrs\brdrw10\brdrcf15 \trbrdrh\brdrs\brdrw10\brdrcf15 \trbrdrv\brdrs\brdrw10\brdrcf15 
\clvertalt\clbrdrt\brdrs\brdrw10\brdrcf15 \clbrdrl\brdrs\brdrw10\brdrcf15 \clbrdrb\brdrs\brdrw10\brdrcf15 \clbrdrr \brdrs\brdrw10\brdrcf15 \cltxlrtb \cellx2187
\clvertalt\clbrdrt\brdrs\brdrw10\brdrcf15 \clbrdrl\brdrs\brdrw10\brdrcf15 \clbrdrb\brdrs\brdrw10\brdrcf15 \clbrdrr \brdrs\brdrw10\brdrcf15 \cltxlrtb \cellx8748
\pard \widctlpar\intbl\adjustright
{{\i round_nearest} \cell }{Round the number of channels in each layer to be a multiple of this number. \cell }
{\row }
\trowd \trgaph108\trleft426\tblind426\trbrdrt\brdrs\brdrw10\brdrcf15 \trbrdrl\brdrs\brdrw10\brdrcf15 \trbrdrb\brdrs\brdrw10\brdrcf15 \trbrdrr\brdrs\brdrw10\brdrcf15 \trbrdrh\brdrs\brdrw10\brdrcf15 \trbrdrv\brdrs\brdrw10\brdrcf15 
\clvertalt\clbrdrt\brdrs\brdrw10\brdrcf15 \clbrdrl\brdrs\brdrw10\brdrcf15 \clbrdrb\brdrs\brdrw10\brdrcf15 \clbrdrr \brdrs\brdrw10\brdrcf15 \cltxlrtb \cellx2187
\clvertalt\clbrdrt\brdrs\brdrw10\brdrcf15 \clbrdrl\brdrs\brdrw10\brdrcf15 \clbrdrb\brdrs\brdrw10\brdrcf15 \clbrdrr \brdrs\brdrw10\brdrcf15 \cltxlrtb \cellx8748
\pard \widctlpar\intbl\adjustright
{{\i dropout} \cell }{Dropout probability for the dropout layer in the classifier. \cell }
{\row }
}
}}
{\pard\widctlpar\brdrb\brdrs\brdrw5\brsp20 \adjustright \par}
\pard\plain \s3\sb240\sa60\keepn\widctlpar\adjustright \b\f1\cgrid 
Member Function Documentation\par
\pard\plain 
{\xe \v forward\:MobileNetV2}
{\xe \v MobileNetV2\:forward}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
torch::Tensor MobileNetV2::forward (torch::Tensor  {\i x}){\f2 [inline]}}}
\par
{\bkmkstart AAAAAAAAAF}
{\bkmkend AAAAAAAAAF}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
\par
{
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Performs the forward pass. }}\par
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
{\par
{\s5\sb90\sa30\keepn\widctlpar\adjustright \b\f1\fs20\cgrid 
Parameters\par}
\pard\plain \s81\li360\widctlpar\ql\adjustright \fs20\cgrid \trowd \trgaph108\trleft426\tblind426\trbrdrt\brdrs\brdrw10\brdrcf15 \trbrdrl\brdrs\brdrw10\brdrcf15 \trbrdrb\brdrs\brdrw10\brdrcf15 \trbrdrr\brdrs\brdrw10\brdrcf15 \trbrdrh\brdrs\brdrw10\brdrcf15 \trbrdrv\brdrs\brdrw10\brdrcf15 
\clvertalt\clbrdrt\brdrs\brdrw10\brdrcf15 \clbrdrl\brdrs\brdrw10\brdrcf15 \clbrdrb\brdrs\brdrw10\brdrcf15 \clbrdrr \brdrs\brdrw10\brdrcf15 \cltxlrtb \cellx2187
\clvertalt\clbrdrt\brdrs\brdrw10\brdrcf15 \clbrdrl\brdrs\brdrw10\brdrcf15 \clbrdrb\brdrs\brdrw10\brdrcf15 \clbrdrr \brdrs\brdrw10\brdrcf15 \cltxlrtb \cellx8748
\pard \widctlpar\intbl\adjustright
{{\i x} \cell }{The batch of input images. \cell }
{\row }
}
{{\s5\sb90\sa30\keepn\widctlpar\adjustright \b\f1\fs20\cgrid 
Returns\par}\pard\plain \s82\li720\widctlpar\ql\adjustright \fs20\cgrid {\s17 \sa60 \sb30
torch::Tensor The category scores for the different labels. \par
}}}}
{\xe \v getNinputChannels4Classifier\:MobileNetV2}
{\xe \v MobileNetV2\:getNinputChannels4Classifier}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
int MobileNetV2::getNinputChannels4Classifier () const{\f2 [inline]}}}
\par
{\bkmkstart AAAAAAAAAG}
{\bkmkend AAAAAAAAAG}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
\par
{
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Gets the number of input channels for the classfier. }}\par
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
This will make it easy to replace the classifier with anything the user wants by creating their own torch::nn::Sequential() for the classifier.\par
{{\s5\sb90\sa30\keepn\widctlpar\adjustright \b\f1\fs20\cgrid 
Returns\par}\pard\plain \s82\li720\widctlpar\ql\adjustright \fs20\cgrid {\s17 \sa60 \sb30
int The number of intput channels to the classifer class "classfier". \par
}}}}
{\xe \v load_weights\:MobileNetV2}
{\xe \v MobileNetV2\:load_weights}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
void MobileNetV2::load_weights (std::string  {\i pt}){\f2 [inline]}}}
\par
{\bkmkstart AAAAAAAAAH}
{\bkmkend AAAAAAAAAH}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
\par
{
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Loads a .pt weight file containing a dict with key/parameter pairs. }}\par
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
See {\f2 https://github.com/pytorch/pytorch/issues/36577}\par
{{\s5\sb90\sa30\keepn\widctlpar\adjustright \b\f1\fs20\cgrid 
Parameters\par}
\pard\plain \s81\li360\widctlpar\ql\adjustright \fs20\cgrid \trowd \trgaph108\trleft426\tblind426\trbrdrt\brdrs\brdrw10\brdrcf15 \trbrdrl\brdrs\brdrw10\brdrcf15 \trbrdrb\brdrs\brdrw10\brdrcf15 \trbrdrr\brdrs\brdrw10\brdrcf15 \trbrdrh\brdrs\brdrw10\brdrcf15 \trbrdrv\brdrs\brdrw10\brdrcf15 
\clvertalt\clbrdrt\brdrs\brdrw10\brdrcf15 \clbrdrl\brdrs\brdrw10\brdrcf15 \clbrdrb\brdrs\brdrw10\brdrcf15 \clbrdrr \brdrs\brdrw10\brdrcf15 \cltxlrtb \cellx2187
\clvertalt\clbrdrt\brdrs\brdrw10\brdrcf15 \clbrdrl\brdrs\brdrw10\brdrcf15 \clbrdrb\brdrs\brdrw10\brdrcf15 \clbrdrr \brdrs\brdrw10\brdrcf15 \cltxlrtb \cellx8748
\pard \widctlpar\intbl\adjustright
{{\i pt} \cell }{filename of the .pt weight file. \cell }
{\row }
}
}}
{\xe \v preprocess\:MobileNetV2}
{\xe \v MobileNetV2\:preprocess}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
static torch::Tensor MobileNetV2::preprocess (cv::Mat  {\i img}, bool  {\i resizeOnly} = {\f2 false}){\f2 [inline]}, {\f2 [static]}}}
\par
{\bkmkstart AAAAAAAAAI}
{\bkmkend AAAAAAAAAI}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
\par
{
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Preprocessing of an openCV image for inference or learning. }}\par
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
The images are resized to 256x256, followed by a central crop of 224x224. Finally the values are first rescaled to [0.0, 1.0] and then normalized using mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225].\par
{{\s5\sb90\sa30\keepn\widctlpar\adjustright \b\f1\fs20\cgrid 
Parameters\par}
\pard\plain \s81\li360\widctlpar\ql\adjustright \fs20\cgrid \trowd \trgaph108\trleft426\tblind426\trbrdrt\brdrs\brdrw10\brdrcf15 \trbrdrl\brdrs\brdrw10\brdrcf15 \trbrdrb\brdrs\brdrw10\brdrcf15 \trbrdrr\brdrs\brdrw10\brdrcf15 \trbrdrh\brdrs\brdrw10\brdrcf15 \trbrdrv\brdrs\brdrw10\brdrcf15 
\clvertalt\clbrdrt\brdrs\brdrw10\brdrcf15 \clbrdrl\brdrs\brdrw10\brdrcf15 \clbrdrb\brdrs\brdrw10\brdrcf15 \clbrdrr \brdrs\brdrw10\brdrcf15 \cltxlrtb \cellx2187
\clvertalt\clbrdrt\brdrs\brdrw10\brdrcf15 \clbrdrl\brdrs\brdrw10\brdrcf15 \clbrdrb\brdrs\brdrw10\brdrcf15 \clbrdrr \brdrs\brdrw10\brdrcf15 \cltxlrtb \cellx8748
\pard \widctlpar\intbl\adjustright
{{\i img} \cell }{8bit BGR openCV image with an aspect ratio of 1:1. \cell }
{\row }
\trowd \trgaph108\trleft426\tblind426\trbrdrt\brdrs\brdrw10\brdrcf15 \trbrdrl\brdrs\brdrw10\brdrcf15 \trbrdrb\brdrs\brdrw10\brdrcf15 \trbrdrr\brdrs\brdrw10\brdrcf15 \trbrdrh\brdrs\brdrw10\brdrcf15 \trbrdrv\brdrs\brdrw10\brdrcf15 
\clvertalt\clbrdrt\brdrs\brdrw10\brdrcf15 \clbrdrl\brdrs\brdrw10\brdrcf15 \clbrdrb\brdrs\brdrw10\brdrcf15 \clbrdrr \brdrs\brdrw10\brdrcf15 \cltxlrtb \cellx2187
\clvertalt\clbrdrt\brdrs\brdrw10\brdrcf15 \clbrdrl\brdrs\brdrw10\brdrcf15 \clbrdrb\brdrs\brdrw10\brdrcf15 \clbrdrr \brdrs\brdrw10\brdrcf15 \cltxlrtb \cellx8748
\pard \widctlpar\intbl\adjustright
{{\i resizeOnly} \cell }{If true the image is only resized to 224x224 but not cropped. Default: false. \cell }
{\row }
}
{{\s5\sb90\sa30\keepn\widctlpar\adjustright \b\f1\fs20\cgrid 
Returns\par}\pard\plain \s82\li720\widctlpar\ql\adjustright \fs20\cgrid {\s17 \sa60 \sb30
torch::Tensor The image as a tensor ready to be used for inference and learning. \par
}}}}
{\pard\widctlpar\brdrb\brdrs\brdrw5\brsp20 \adjustright \par}
\pard\plain \s3\sb240\sa60\keepn\widctlpar\adjustright \b\f1\cgrid 
Member Data Documentation\par
\pard\plain 
{\xe \v classifier\:MobileNetV2}
{\xe \v MobileNetV2\:classifier}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
torch::nn::Sequential MobileNetV2::classifier}}
\par
{\bkmkstart AAAAAAAAAJ}
{\bkmkend AAAAAAAAAJ}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
\par
{
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Classifier submodule. }}\par
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
This can be replaced by a custom classifier for transfer learning. \par
}}
{\xe \v classifierModuleName\:MobileNetV2}
{\xe \v MobileNetV2\:classifierModuleName}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
constexpr char MobileNetV2::classifierModuleName[] = "classifier"{\f2 [static]}, {\f2 [constexpr]}}}
\par
{\bkmkstart AAAAAAAAAK}
{\bkmkend AAAAAAAAAK}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
\par
{
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Name of the classifier module. }}\par
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
The name of the classifier module is needed whne replacing the default classifier. \par
}}
{\xe \v features\:MobileNetV2}
{\xe \v MobileNetV2\:features}
\pard\plain \s4\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs20\cgrid {
{\b 
torch::nn::Sequential MobileNetV2::features}}
\par
{\bkmkstart AAAAAAAAAL}
{\bkmkend AAAAAAAAAL}
{
\pard\plain \s61\li360\sa60\sb30\qj\widctlpar\qj\adjustright \fs20\cgrid 
\par
{
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
Features submodule. }}\par
{\s17\sa60\sb30\widctlpar\qj \fs22\cgrid 
This needs to be accessible for transfer learning which then will disable learning here. \par
}}
{\pard\widctlpar\brdrb\brdrs\brdrw5\brsp20 \adjustright \par}
The documentation for this class was generated from the following file:{\par
\pard\plain \s121\fi-360\li720\widctlpar\jclisttab\tx720{\*\pn \pnlvlbody\ilvl0\ls2\pnrnot0\pndec }\ls1\adjustright \fs20\cgrid 
mobilenet_v2.h\par
}
\pard\plain \sect\sbkpage
\s1\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs36\kerning36\cgrid 
File Documentation{\tc \v File Documentation}
\par \pard\plain 
\pard\plain \s2\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs28\kerning28\cgrid 
mobilenet_v2.h\par \pard\plain 
{\bkmkstart AAAAAAAAAA}
{\bkmkend AAAAAAAAAA}
{
\pard\plain \s40\li0\widctlpar\adjustright \shading1000\cbpat8 \f2\fs16\cgrid 1 {\cf21 #pragma once}\par
2 \par
3 {\cf21 #include <torch/torch.h>}\par
4 {\cf21 #include <vector>}\par
5 {\cf21 #include <algorithm>}\par
6 {\cf21 #include <fstream>}\par
7 {\cf21 #include <string>}\par
8 {\cf21 #include <regex>}\par
9 {\cf21 #include <filesystem>}\par
10 {\cf21 #include <iostream>}\par
11 {\cf21 #include <system_error>}\par
12 {\cf21 #include <opencv2/opencv.hpp>}\par
13 \par
14 {\cf20 /***}\par
15 {\cf20  * MobileNetV2 C++ Implementation (LibTorch).}\par
16 {\cf20  * It's able to load pre-trained weights from torchvision.}\par
17 {\cf20  * (c) 2025 Bernd Porr, GPLv3.}\par
18 {\cf20  ***/}\par
19 \par
20 {\cf21 #ifdef NDEBUG}\par
21 {\cf17 constexpr} {\cf18 bool} debugOutput = {\cf17 false};\par
22 {\cf21 #else}\par
23 {\cf17 constexpr} {\cf18 bool} debugOutput = {\cf17 true};\par
24 {\cf21 #endif}\par
25 \par
30 {\cf17 class }MobileNetV2 : {\cf17 public} torch::nn::Module\par
31 \{\par
32 {\cf17 public}:\par
33     {\cf20 // Module name of the feature detector.}\par
34     {\cf17 static} {\cf17 constexpr} {\cf18 char} featuresModuleName[] = {\cf22 "features"};\par
35 \par
41     {\cf17 static} {\cf17 constexpr} {\cf18 char} classifierModuleName[] = {\cf22 "classifier"};\par
42 \par
53     MobileNetV2({\cf18 int} num_classes = 1000, {\cf18 float} width_mult = 1.0f, {\cf18 int} round_nearest = 8, {\cf18 float} dropout = 0.2)\par
54     \{\par
55         {\cf18 int} input_channels = 32;\par
56         input_channels = make_divisible(input_channels * width_mult, round_nearest);\par
57         features_output_channels = make_divisible(features_output_channels * std::max(1.0f, width_mult), round_nearest);\par
58 \par
59         features = torch::nn::Sequential();\par
60 \par
61         features->push_back(\par
62             Conv2dNormActivation(3,\par
63                                  input_channels,\par
64                                  {\cf20 /*kernel_size=*/}3,\par
65                                  {\cf20 /*stride =*/}2));\par
66 \par
67         {\cf20 // inverted residual blocks}\par
68         {\cf19 for} ({\cf17 const} {\cf17 auto} &cfg : inverted_residual_setting)\par
69         \{\par
70             {\cf17 const} {\cf18 int} t = cfg[0];\par
71             {\cf17 const} {\cf18 int} c = cfg[1];\par
72             {\cf17 const} {\cf18 int} n = cfg[2];\par
73             {\cf17 const} {\cf18 int} s = cfg[3];\par
74 \par
75             {\cf18 int} output_channel = make_divisible(c * width_mult, round_nearest);\par
76             {\cf19 for} ({\cf18 int} i = 0; i < n; ++i)\par
77             \{\par
78                 {\cf17 const} {\cf18 int} stride = (i == 0) ? s : 1;\par
79                 features->push_back(\par
80                     InvertedResidual(input_channels, output_channel, stride, t));\par
81                 input_channels = output_channel;\par
82             \}\par
83         \}\par
84 \par
85         features->push_back(\par
86             Conv2dNormActivation(input_channels,\par
87                                  features_output_channels,\par
88                                  {\cf20 /*kernel_size=*/}1));\par
89 \par
90         register_module(featuresModuleName, features);\par
91 \par
92         {\cf20 // classifier: Dropout + Linear}\par
93         classifier = torch::nn::Sequential();\par
94         classifier->push_back(torch::nn::Dropout(torch::nn::DropoutOptions(dropout)));\par
95         classifier->push_back(torch::nn::Linear(torch::nn::LinearOptions(features_output_channels, num_classes)));\par
96         register_module(classifierModuleName, classifier);\par
97     \}\par
98 \par
106     {\cf18 int} getNinputChannels4Classifier(){\cf17  const}\par
107 {\cf17     }\{\par
108         {\cf19 return} features_output_channels;\par
109     \}\par
110 \par
117     torch::Tensor forward(torch::Tensor x)\par
118     \{\par
119         x = features->forward(x);\par
120         {\cf17 const} torch::nn::functional::AdaptiveAvgPool2dFuncOptions &ar = torch::nn::functional::AdaptiveAvgPool2dFuncOptions(\{1, 1\});\par
121         x = torch::nn::functional::adaptive_avg_pool2d(x, ar);\par
122         x = torch::flatten(x, 1);\par
123         x = classifier->forward(x);\par
124         {\cf19 return} x;\par
125     \}\par
126 \par
130     {\cf18 void} initialize_weights()\par
131     \{\par
132         {\cf19 for} ({\cf17 auto} &module : modules({\cf20 /*include_self=*/}{\cf17 false}))\par
133         \{\par
134             {\cf19 if} ({\cf17 auto} M = {\cf17 dynamic_cast<}torch::nn::Conv2dImpl *{\cf17 >}(module.get()))\par
135             \{\par
136                 torch::nn::init::kaiming_normal_(M->weight, {\cf20 /*a=*/}0, torch::kFanOut, torch::kReLU);\par
137                 {\cf19 if} (M->options.bias())\par
138                     torch::nn::init::zeros_(M->bias);\par
139             \}\par
140             {\cf19 else} {\cf19 if} ({\cf17 auto} M = {\cf17 dynamic_cast<}torch::nn::BatchNorm2dImpl *{\cf17 >}(module.get()))\par
141             \{\par
142                 torch::nn::init::ones_(M->weight);\par
143                 torch::nn::init::zeros_(M->bias);\par
144             \}\par
145             {\cf19 else} {\cf19 if} ({\cf17 auto} M = {\cf17 dynamic_cast<}torch::nn::LinearImpl *{\cf17 >}(module.get()))\par
146             \{\par
147                 torch::nn::init::normal_(M->weight, 0.0, 0.01);\par
148                 torch::nn::init::zeros_(M->bias);\par
149             \}\par
150         \}\par
151     \}\par
152 \par
159     {\cf18 void} load_weights(std::string pt)\par
160     \{\par
161         std::ifstream input(pt, std::ios::binary);\par
162         input.exceptions(input.failbit);\par
163         std::vector<char> bytes(\par
164             (std::istreambuf_iterator<char>(input)),\par
165             (std::istreambuf_iterator<char>()));\par
166         input.close();\par
167         {\cf17 const} c10::Dict<c10::IValue, c10::IValue> weights = torch::pickle_load(bytes).toGenericDict();\par
168         {\cf19 if} (debugOutput)\par
169         \{\par
170             std::cerr << {\cf22 "Parameters we have in this model here: "} << std::endl;\par
171             {\cf19 for} ({\cf17 auto} {\cf17 const} &m : named_parameters())\par
172             \{\par
173                 {\cf17 auto} k = ourkey2torchvision(m.key());\par
174                 std::cerr << m.key() << {\cf22 "->"} << k << {\cf22 ": "} << m.value().sizes() << std::endl;\par
175             \}\par
176             std::cerr << {\cf22 "Named buffers we have in this model here: "} << std::endl;\par
177             {\cf19 for} ({\cf17 const} {\cf17 auto} &b : named_buffers())\par
178             \{\par
179                 {\cf17 auto} k = ourkey2torchvision(b.key());\par
180                 std::cout << b.key() << {\cf22 "->"} << k << {\cf22 ": "} << b.value().sizes() << std::endl;\par
181             \}\par
182             std::cerr << {\cf22 "Parameters we have in the weight file "} << pt << {\cf22 ":"} << std::endl;\par
183             {\cf19 for} ({\cf17 auto} {\cf17 const} &w : weights)\par
184             \{\par
185                 std::cerr << w.key() << {\cf22 ": "} << w.value().toTensor().sizes() << std::endl;\par
186             \}\par
187         \}\par
188         torch::NoGradGuard no_grad;\par
189         {\cf19 if} (debugOutput)\par
190             std::cerr << {\cf22 "Loading weights"} << std::endl;\par
191         {\cf19 for} ({\cf17 auto} &m : named_parameters())\par
192         \{\par
193             {\cf17 const} std::string model_key = m.key();\par
194             {\cf17 const} std::string model_key4torchvision = ourkey2torchvision(model_key);\par
195             {\cf19 if} (debugOutput)\par
196                 std::cerr << {\cf22 "Searching for: "} << model_key4torchvision << {\cf22 ": "} << m.value().sizes() << std::endl;\par
197             {\cf18 bool} foundit = {\cf17 false};\par
198             {\cf19 for} ({\cf17 auto} {\cf17 const} &w : weights)\par
199             \{\par
200                 {\cf19 if} (model_key4torchvision == w.key())\par
201                 \{\par
202                     {\cf19 if} (debugOutput)\par
203                         std::cerr << {\cf22 "Found it: "} << w.key() << std::endl;\par
204                     m.value().copy_(w.value().toTensor());\par
205                     foundit = {\cf17 true};\par
206                     {\cf19 break};\par
207                 \}\par
208             \}\par
209             {\cf19 if} (!foundit)\par
210                 std::cerr << {\cf22 "Key: "} << model_key4torchvision << {\cf22 " could not be found!"} << std::endl;\par
211         \}\par
212         {\cf19 if} (debugOutput)\par
213             std::cerr << {\cf22 "Loading named buffers"} << std::endl;\par
214         {\cf19 for} ({\cf17 auto} &b : named_buffers())\par
215         \{\par
216             std::string model_key = b.key();\par
217             std::string model_key4torchvision = ourkey2torchvision(model_key);\par
218             {\cf19 if} (debugOutput)\par
219                 std::cerr << {\cf22 "Searching for: "} << model_key4torchvision << {\cf22 ": "} << b.value().sizes() << std::endl;\par
220             {\cf18 bool} foundit = {\cf17 false};\par
221             {\cf19 for} ({\cf17 auto} {\cf17 const} &w : weights)\par
222             \{\par
223                 {\cf19 if} (model_key4torchvision == w.key())\par
224                 \{\par
225                     {\cf19 if} (debugOutput)\par
226                         std::cerr << {\cf22 "Found it: "} << w.key() << std::endl;\par
227                     b.value().copy_(w.value().toTensor());\par
228                     foundit = {\cf17 true};\par
229                     {\cf19 break};\par
230                 \}\par
231             \}\par
232             {\cf19 if} (!foundit)\par
233                 std::cerr << {\cf22 "Key: "} << model_key4torchvision << {\cf22 " could not be found!"} << std::endl;\par
234         \}\par
235     \}\par
236 \par
247     {\cf17 static} torch::Tensor preprocess(cv::Mat img, {\cf18 bool} resizeOnly = {\cf17 false})\par
248     \{\par
249         {\cf17 constexpr} {\cf18 int} imageSizeBeforeCrop = 256;\par
250         {\cf17 constexpr} {\cf18 int} finalImageSize = 224;\par
251         {\cf17 constexpr} {\cf18 int} numChannels = 3; {\cf20 // colour}\par
252 \par
253         {\cf19 if} (img.depth() != CV_8U)\par
254             {\cf19 throw} std::invalid_argument({\cf22 "Image is not 8bit."});\par
255         {\cf19 if} (img.channels() != numChannels)\par
256             {\cf19 throw} std::invalid_argument({\cf22 "Image is not BGR / colour."});\par
257 \par
258         {\cf19 if} (resizeOnly)\par
259         \{\par
260             cv::resize(img, img, cv::Size(finalImageSize, finalImageSize));\par
261         \}\par
262         {\cf19 else}\par
263         \{\par
264             cv::resize(img, img, cv::Size(imageSizeBeforeCrop, imageSizeBeforeCrop));\par
265             {\cf17 constexpr} {\cf18 int} start = (imageSizeBeforeCrop - finalImageSize) / 2;\par
266             {\cf17 const} cv::Rect roi(start, start, finalImageSize, finalImageSize);\par
267             img = img(roi).clone();\par
268         \}\par
269         cv::cvtColor(img, img, cv::COLOR_BGR2RGB);\par
270 \par
271         torch::Tensor tensor = torch::from_blob(img.data, \{img.rows, img.cols, 3\}, torch::kByte);\par
272         tensor = tensor.permute(\{2, 0, 1\}).to(torch::kFloat).div_(255.0);\par
273         tensor = torch::data::transforms::Normalize(\{0.485, 0.456, 0.406\}, \{0.229, 0.224, 0.225\})(tensor);\par
274         {\cf19 return} tensor;\par
275     \}\par
276 \par
281     torch::nn::Sequential classifier;\par
282 \par
287     torch::nn::Sequential features;\par
288 \par
289 {\cf17 private}:\par
290     {\cf20 // Helper which maps the libtorch keys to pytorch keys.}\par
291     std::string ourkey2torchvision(std::string k){\cf17  const}\par
292 {\cf17     }\{\par
293         {\cf20 // called simply "conv" in the weights file}\par
294         k = std::regex_replace(k, std::regex(InvertedResidual::className), {\cf22 "conv"});\par
295         {\cf20 // not used at all in the weights file}\par
296         {\cf17 const} std::string r = std::string(Conv2dNormActivation::className) + {\cf22 "\\\\."};\par
297         k = std::regex_replace(k, std::regex(r), {\cf22 ""});\par
298         {\cf19 return} k;\par
299     \}\par
300 \par
301     {\cf20 // Makes a value divisible.}\par
302     {\cf17 inline} {\cf18 int} make_divisible({\cf18 int} v, {\cf18 int} divisor = 8, {\cf18 int} min_value = -1){\cf17  const}\par
303 {\cf17     }\{\par
304         {\cf19 if} (min_value < 0)\par
305             min_value = divisor;\par
306         {\cf18 int} new_v = std::max(min_value, (({\cf18 int})((({\cf18 int})(v + divisor / 2)) / divisor)) * divisor);\par
307         {\cf19 if} (new_v < (0.9 * ({\cf18 float})v))\par
308             new_v += divisor;\par
309         {\cf19 return} new_v;\par
310     \}\par
311 \par
312     {\cf20 // Features output channels but can be scaled.}\par
313     {\cf18 int} features_output_channels = 1280;\par
314 \par
315     {\cf20 // MobileNetV2 inverted residual settings:}\par
316     {\cf20 // t, c, n, s  (expansion, output channels, repeats, stride)}\par
317     {\cf17 const} std::vector<std::array<int, 4>> inverted_residual_setting = \{\par
318         \{1, 16, 1, 1\},\par
319         \{6, 24, 2, 2\},\par
320         \{6, 32, 3, 2\},\par
321         \{6, 64, 4, 2\},\par
322         \{6, 96, 3, 1\},\par
323         \{6, 160, 3, 2\},\par
324         \{6, 320, 1, 1\},\par
325     \};\par
326 \par
331     {\cf17 class }Conv2dNormActivation : {\cf17 public} torch::nn::Module\par
332     \{\par
333     {\cf17 public}:\par
334         {\cf17 static} {\cf17 constexpr} {\cf18 char} className[] = {\cf22 "Conv2dNormActivation"};\par
335 \par
336         {\cf17 static} {\cf17 inline} torch::Tensor relu6({\cf17 const} torch::Tensor &x)\par
337         \{\par
338             {\cf19 return} torch::clamp(torch::relu(x), 0, 6);\par
339         \}\par
340 \par
341         Conv2dNormActivation({\cf18 int} in_channels,\par
342                              {\cf18 int} out_channels,\par
343                              {\cf18 int} kernel_size = 3,\par
344                              {\cf18 int} stride = 1,\par
345                              {\cf18 int} padding = -1,\par
346                              {\cf18 int} groups = 1)\par
347         \{\par
348             {\cf17 const} {\cf18 int} dilation = 1;\par
349             conv = torch::nn::Sequential();\par
350             {\cf19 if} (padding < 0)\par
351             \{\par
352                 padding = (kernel_size - 1) / 2 * dilation;\par
353             \}\par
354             conv->push_back(torch::nn::Conv2d(\par
355                 torch::nn::Conv2dOptions(in_channels, out_channels, kernel_size)\par
356                     .stride(stride)\par
357                     .padding(padding)\par
358                     .groups(groups)\par
359                     .bias({\cf17 false})));\par
360             conv->push_back(torch::nn::BatchNorm2d(torch::nn::BatchNorm2dOptions(out_channels)));\par
361             conv->push_back(torch::nn::Functional(relu6));\par
362             register_module(className, conv);\par
363         \}\par
364 \par
365         torch::Tensor forward({\cf17 const} torch::Tensor &x)\par
366         \{\par
367             {\cf19 return} conv->forward(x);\par
368         \}\par
369 \par
370     {\cf17 private}:\par
371         torch::nn::Sequential conv\{{\cf17 nullptr}\};\par
372     \};\par
373 \par
378     {\cf17 class }InvertedResidual : {\cf17 public} torch::nn::Module\par
379     \{\par
380     {\cf17 public}:\par
381         {\cf17 static} {\cf17 constexpr} {\cf18 char} className[] = {\cf22 "InvertedResidual"};\par
382 \par
383         InvertedResidual({\cf18 int} inp, {\cf18 int} oup, {\cf18 int} stride, {\cf18 int} expand_ratio)\par
384         \{\par
385             {\cf19 if} ((stride < 1) || (stride > 2))\par
386             \{\par
387                 {\cf19 throw} std::invalid_argument({\cf22 "Stride needs to be 1 or 2."});\par
388             \}\par
389             {\cf17 const} {\cf18 int} hidden_dim = (int)round(inp * expand_ratio);\par
390             use_res_connect = (stride == 1) && (inp == oup);\par
391 \par
392             conv = torch::nn::Sequential();\par
393 \par
394             {\cf19 if} (expand_ratio != 1)\par
395             \{\par
396                 conv->push_back(\par
397                     Conv2dNormActivation(inp,\par
398                                          hidden_dim,\par
399                                          {\cf20 /*kernel_size*/} 1));\par
400             \}\par
401 \par
402             conv->push_back(\par
403                 Conv2dNormActivation(hidden_dim,\par
404                                      hidden_dim,\par
405                                      {\cf20 /*kernel_size=*/}3,\par
406                                      {\cf20 /*stride=*/}stride,\par
407                                      {\cf20 /*padding=*/}-1,\par
408                                      {\cf20 /*groups=*/}hidden_dim));\par
409 \par
410             conv->push_back(torch::nn::Conv2d(\par
411                 torch::nn::Conv2dOptions(hidden_dim, oup,\par
412                                          {\cf20 /*kernel_size=*/}1)\par
413                     .stride(1)\par
414                     .padding(0)\par
415                     .bias({\cf17 false})));\par
416             conv->push_back(torch::nn::BatchNorm2d(torch::nn::BatchNorm2dOptions(oup)));\par
417 \par
418             register_module(className, conv);\par
419         \}\par
420 \par
421         torch::Tensor forward({\cf17 const} torch::Tensor &x)\par
422         \{\par
423             {\cf19 if} (use_res_connect)\par
424             \{\par
425                 {\cf19 return} x + conv->forward(x);\par
426             \}\par
427             {\cf19 else}\par
428             \{\par
429                 {\cf19 return} conv->forward(x);\par
430             \}\par
431         \}\par
432         torch::nn::Sequential conv\{{\cf17 nullptr}\};\par
433         {\cf18 bool} use_res_connect;\par
434     \};\par
435 \};\par
}

\pard\plain \sect\sbkpage
\s1\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs36\kerning36\cgrid 
\s1\sb240\sa60\keepn\widctlpar\adjustright \b\f1\fs36\kerning36\cgrid Index\par 
\pard\plain 
{\tc \v Index}
{\field\fldedit {\*\fldinst INDEX \\c2 \\*MERGEFORMAT}{\fldrslt INDEX}}
}
